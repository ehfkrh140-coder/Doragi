import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import google.generativeai as genai
import urllib.parse
import random
import email.utils
from datetime import datetime, timedelta

# ==========================================
# ğŸ”‘ [í•„ìˆ˜] Gemini API í‚¤ ì„¤ì •
# ==========================================
try:
    GOOG_API_KEY = st.secrets["GOOG_API_KEY"]
except:
    GOOG_API_KEY = "ì—¬ê¸°ì—_í‚¤ë¥¼_ë„£ìœ¼ì„¸ìš”"

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì£¼ì‹ í…Œë§ˆ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ¤– AI ì£¼ì‹ íˆ¬ì ì „ëµê°€ (7-Day Context Ver.)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_code" not in st.session_state:
    st.session_state.last_code = None
if "current_news_data" not in st.session_state:
    st.session_state.current_news_data = [] 
if "current_market_fact" not in st.session_state:
    st.session_state.current_market_fact = ""

# --- ì•ˆì „ í•„í„° í•´ì œ (ì „ì—­ ë³€ìˆ˜) ---
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# --- [ëª¨ë¸ ëª©ë¡] ---
@st.cache_data(ttl=3600)
def get_available_gemini_models(api_key):
    try:
        genai.configure(api_key=api_key)
        return [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except: return ["gemini-1.5-flash"]

# --- [í•µì‹¬] Google News RSS ìˆ˜ì§‘ê¸° (7ì¼ + ìµœì‹ ìˆœ ì •ë ¬) ---
def fetch_google_news_rss(keyword, limit=30):
    news_data = []
    try:
        # [ìˆ˜ì •] when:7d (ìµœê·¼ 7ì¼)ë¡œ ê¸°ê°„ í™•ì¥
        search_query = f"{keyword} when:7d"
        encoded_kw = urllib.parse.quote(search_query)
        
        # [ìœ ì§€] scoring=n (ìµœì‹ ìˆœ ì •ë ¬)ì€ í•„ìˆ˜
        url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=ko&gl=KR&ceid=KR:ko&scoring=n&t={int(time.time())}"
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        
        if res.status_code == 200:
            soup = BeautifulSoup(res.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                title = item.title.text
                link = item.link.text
                pub_date = item.pubDate.text
                raw_desc = item.description.text
                clean_desc = BeautifulSoup(raw_desc, "html.parser").get_text(separator=" ", strip=True)
                
                # [ìˆ˜ì •] Python ë ˆë²¨ í•„í„°ë§ë„ 7ì¼(days=7)ë¡œ ë³€ê²½
                dt_object = None
                try:
                    dt_object = email.utils.parsedate_to_datetime(pub_date)
                    now = datetime.now(dt_object.tzinfo)
                    if dt_object < (now - timedelta(days=7)): 
                        continue
                except:
                    pass

                source = "News"
                if "-" in title:
                    parts = title.rsplit("-", 1)
                    if len(parts) > 1:
                        source = parts[1].strip()
                        title = parts[0].strip()
                    
                news_data.append({
                    "source": source, "title": title, "link": link,
                    "summary": clean_desc, "date": pub_date, "date_obj": dt_object
                })
                
            # ìµœì‹ ìˆœ ì •ë ¬ ë³´ì¥ (7ì¼ì¹˜ ë°ì´í„° ì¤‘ì—ì„œë„ ìµœì‹ ì´ ìœ„ë¡œ)
            news_data.sort(key=lambda x: x['date_obj'] if x['date_obj'] else datetime.min.replace(tzinfo=None), reverse=True)
            news_data = news_data[:limit]

    except Exception as e:
        print(f"RSS Error: {e}")
    return news_data

# --- [ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ (10ë¶„ ê°±ì‹ )] ---
@st.cache_data(ttl=600)
def get_top_50_themes_stocks():
    url = "https://finance.naver.com/sise/theme.naver"
    all_theme_stocks = [] 
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        
        theme_links = []
        for row in soup.select("#contentarea_left > table.type_1 > tr"):
            cols = row.select("td")
            if len(cols) >= 4:
                theme_name = cols[0].text.strip()
                link = "https://finance.naver.com" + cols[0].find('a')['href']
                theme_links.append({"name": theme_name, "link": link})
                if len(theme_links) >= 50: break
        
        for idx, theme in enumerate(theme_links):
            try:
                res_t = requests.get(theme['link'], headers={'User-Agent': 'Mozilla/5.0'})
                soup_t = BeautifulSoup(res_t.content.decode('cp949', 'ignore'), 'html.parser')
                
                inner_rank = 1
                for row in soup_t.select("table.type_5 > tbody > tr"):
                    cols = row.select("td")
                    if len(cols) > 4:
                        name_tag = cols[0].find('a')
                        if not name_tag: continue
                        stock_name = name_tag.text.strip()
                        link_sub = name_tag['href']
                        code_match = re.search(r'code=([0-9]+)', link_sub)
                        code = code_match.group(1) if code_match else ""
                        price_str = cols[2].text.strip() + " (" + cols[4].text.strip().replace('\n', '').strip() + ")"
                        
                        rank_display = f"ğŸ‘‘ {inner_rank}ìœ„" if inner_rank == 1 else f"{inner_rank}ìœ„"
                        
                        all_theme_stocks.append({
                            "code": code, "ì¢…ëª©ëª…": stock_name, "í…Œë§ˆëª…": theme['name'],
                            "í…Œë§ˆìˆœìœ„": f"{idx+1}ìœ„", "í…Œë§ˆìˆœìœ„_int": idx+1,
                            "í…Œë§ˆë‚´ìˆœìœ„": rank_display,
                            "í˜„ì¬ê°€(ë“±ë½ë¥ )": price_str
                        })
                        inner_rank += 1
            except: pass
    except: pass
    return pd.DataFrame(all_theme_stocks)

@st.cache_data(ttl=600)
def get_risers_data_with_market():
    riser_map = {}
    for s, market_name in [(0, "KOSPI"), (1, "KOSDAQ")]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_rise.naver?sosok={s}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for item in soup.select("table.type_2 tr td a.tltle"):
                link = item['href']
                code_match = re.search(r'code=([0-9]+)', link)
                if code_match:
                    code = code_match.group(1)
                    riser_map[code] = market_name
        except: pass
    return riser_map

@st.cache_data(ttl=600)
def get_top_gainers_df(limit=150):
    kospi_gainers = []
    kosdaq_gainers = []
    for market_code, result_list in [(0, kospi_gainers), (1, kosdaq_gainers)]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_rise.naver?sosok={market_code}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            rows = soup.select("table.type_2 tr")
            count = 0
            for row in rows:
                cols = row.select("td")
                if len(cols) < 5: continue
                name_tag = cols[1].find('a')
                if not name_tag: continue
                result_list.append({
                    "ì¢…ëª©ëª…": name_tag.text.strip(),
                    "í˜„ì¬ê°€": cols[2].text.strip(),
                    "ë“±ë½ë¥ ": cols[4].text.strip().replace('\n', '').strip()
                })
                count += 1
                if count >= limit: break
        except: pass
    return pd.DataFrame(kospi_gainers), pd.DataFrame(kosdaq_gainers)

@st.cache_data(ttl=600)
def get_money_flow_codes():
    mf_codes = set()
    headers = {'User-Agent': 'Mozilla/5.0'}
    for s in [0, 1]:
        for page in range(1, 6):
            try:
                url = f"https://finance.naver.com/sise/sise_market_sum.naver?sosok={s}&sort=amount&page={page}"
                res = requests.get(url, headers=headers)
                soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
                items = soup.select("table.type_2 tbody tr td:nth-child(2) a")
                for item in items:
                    link = item['href']
                    code_match = re.search(r'code=([0-9]+)', link)
                    if code_match:
                        mf_codes.add(code_match.group(1))
            except: pass
            time.sleep(0.1)
    return mf_codes

def get_stock_fundamentals(code):
    try:
        url = f"https://finance.naver.com/item/main.naver?code={code}"
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        cap_elem = soup.select_one("#_market_sum")
        if cap_elem:
            raw_cap = cap_elem.text.strip()
            raw_cap = re.sub(r'[è­°å…†]', 'ì¡°', raw_cap)
            raw_cap = raw_cap.replace('\t', '').replace('\n', '').replace('  ', ' ') + "ì–µ"
            return {"ì‹œê°€ì´ì•¡": raw_cap}
    except: pass
    return {"ì‹œê°€ì´ì•¡": "-"}

@st.cache_data(ttl=600)
def get_market_cap_top150():
    stocks = []
    for page in range(1, 4):
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page={page}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for row in soup.select("table.type_2 tbody tr"):
                cols = row.select("td")
                if len(cols) < 10: continue
                stocks.append({
                    "ìˆœìœ„": cols[0].text.strip(), "ì¢…ëª©ëª…": cols[1].text.strip(),
                    "í˜„ì¬ê°€": cols[2].text.strip(), "ë“±ë½ë¥ ": cols[4].text.strip().replace("\n", "").strip(),
                    "ì‹œê°€ì´ì•¡": cols[6].text.strip()
                })
        except: pass
    return pd.DataFrame(stocks)

# --- [AI ì‘ë‹µ í•¨ìˆ˜ 1: ê°œë³„ ì¢…ëª©] ---
def get_gemini_response_stock_deep(messages, model_name, stock_name, theme, market_data_str, news_data):
    genai.configure(api_key=GOOG_API_KEY)
    
    combined_news_context = ""
    if news_data:
        for i, item in enumerate(news_data):
            combined_news_context += f"[{i+1}. {item['source']}] {item['title']} ({item['date']})\n> ìš”ì•½: {item['summary']}\n\n"
    else:
        combined_news_context = "(ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œì¥ ë°ì´í„° ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.)"
        
    search_res = f"""
    \n[í˜„ì¬ ë¶„ì„ ì¤‘ì¸ ì¢…ëª© ë°ì´í„°]
    1. ğŸ“Š ì‹œì¥ íŒ©íŠ¸: {market_data_str}
    2. ğŸ“° ë‰´ìŠ¤ ë°ì´í„° (ì´ {len(news_data)}ê±´):
    {combined_news_context}
    """
    
    sys_instructions = """
    [Role]
    ë‹¹ì‹ ì€ ëƒ‰ì² í•œ íŒë‹¨ë ¥ì„ ê°€ì§„ ì„¸ê³„ìµœê³  ì£¼ì‹ ì• ë„ë¦¬ìŠ¤íŠ¸ ê²¸ ë¶„ì„ê°€ ì…ë‹ˆë‹¤.
    ì œê³µëœ [ì •ëŸ‰ ë°ì´í„°]ì™€ [ë‰´ìŠ¤ ë°ì´í„°]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì‹­ì‹œì˜¤.
    
    [Instruction]
    - ì§ˆë¬¸ì´ "ë¶„ì„í•´ì¤˜" ê°™ì€ ìš”ì²­ì´ë©´ ì•„ë˜ í¬ë§·ìœ¼ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    - ê¸´ë§í•˜ì§€ ë§ê³  í•µì‹¬ë§Œ ëª…í™•í•˜ê²Œ ì „ë‹¬í•˜ì„¸ìš”.
    
    [Report Format]
    ### 1. ğŸ¯ AI íˆ¬ì ë§¤ë ¥ë„ ì ìˆ˜ (100ì  ë§Œì )
    * **ì ìˆ˜:** OOOì 
    * **í•œì¤„ í‰:** (ìƒìŠ¹ ë™ë ¥ ë° ë¦¬ìŠ¤í¬ ìš”ì•½)
    
    ### 2. ğŸš€ í•µì‹¬ ìƒìŠ¹ ë™ë ¥ (Fact Base)
    * ë‰´ìŠ¤ì—ì„œ í™•ì¸ëœ ì‹¤ì²´ ìˆëŠ” í˜¸ì¬ 3ê°€ì§€ë¥¼ ìš”ì•½.
    
    ### 3. âš ï¸ ë¦¬ìŠ¤í¬ ë° ìˆ˜ê¸‰ ì ê²€
    * ê³¼ì—´ ì—¬ë¶€, ëŒ€ì£¼ì£¼ ë§¤ë„, í…Œë§ˆ ëŒ€ì¥ì£¼ ì—¬ë¶€ ë“± íŒë‹¨.
    
    ### 4. ğŸ’¡ ì‹¤ì „ ë§¤ë§¤ ì „ëµ
    * **í¬ì§€ì…˜:** [ì ê·¹ ë§¤ìˆ˜ / ëˆŒë¦¼ëª© ë§¤ìˆ˜ / ê´€ë§ / ë§¤ë„]
    * **ì „ëµ:** êµ¬ì²´ì ì¸ ì§„ì…/ëŒ€ì‘ ê°€ì´ë“œ.
    """
    
    modified_msgs = []
    for msg in messages[:-1]:
        modified_msgs.append({"role": "user" if msg['role']=="user" else "model", "parts": [msg['content']]})
    
    last_content = messages[-1]['content'] + search_res + "\n\n" + sys_instructions
    modified_msgs.append({"role": "user", "parts": [last_content]})
    
    model = genai.GenerativeModel(f"models/{model_name}")
    try:
        response = model.generate_content(modified_msgs, stream=True, safety_settings=safety_settings)
        for chunk in response:
            try:
                if chunk.text: yield chunk.text
            except ValueError: pass
    except Exception as e:
        yield f"âš ï¸ ì‘ë‹µ ì¤‘ ì˜¤ë¥˜: {str(e)}"

# --- [AI ì‘ë‹µ í•¨ìˆ˜ 2: ì‹œí™© ë¶„ì„] ---
def analyze_market_macro_v2(df_cap, df_gainers_kospi, df_gainers_kosdaq, news_data, model_name):
    genai.configure(api_key=GOOG_API_KEY)
    model = genai.GenerativeModel(f"models/{model_name}")
    
    str_cap = df_cap.head(50).to_string(index=False)
    str_kospi_gain = df_gainers_kospi.head(50).to_string(index=False)
    str_kosdaq_gain = df_gainers_kosdaq.head(50).to_string(index=False)
    
    combined_news = ""
    for item in news_data:
        combined_news += f"[{item['source']}] {item['title']}\n(ìš”ì•½): {item['summary']}\n\n"
    
    prompt = f"""
    ë‹¹ì‹ ì€ ê±°ì‹œê²½ì œì™€ ì‹œì¥ íë¦„ì„ ì½ëŠ” êµ­ë‚´ ìµœê³  'ë§ˆì¼“ìŠ¤íŠ¸ë˜í‹°ì§€ìŠ¤íŠ¸ê²¸ ì• ë„ë¦¬ìŠ¤íŠ¸ ì…ë‹ˆë‹¤.
    ê¸´ë§í•˜ì§€ë§ê³  ë°”ë¡œ ë¶„ì„ì— ë“¤ì–´ê°€ ì£¼ì„¸ìš”.
    
    [ì…ë ¥ ë°ì´í„°]
    1. Blue Chips (50ìœ„): {str_cap}
    2. Momentum (ê¸‰ë“±ì£¼): {str_kospi_gain} / {str_kosdaq_gain}
    3. News: {combined_news}
    
    [ë¶„ì„ ìš”êµ¬ì‚¬í•­]
    ìœ„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ 'ëŒ€í˜•ì£¼(ì§€ìˆ˜)'ì™€ 'ê°œë³„ ê¸‰ë“±ì£¼(í…Œë§ˆ)'ì˜ ê´´ë¦¬ë¥¼ ë¶„ì„í•˜ê³ ,
    ì˜¤ëŠ˜ ì‹œì¥ì˜ **'ì§„ì§œ ì£¼ë„ íë¦„'**ì„ ëª…í™•íˆ ì •ì˜í•´ ì£¼ì„¸ìš”.
    
    ### 1. ğŸŒ ì˜¤ëŠ˜ì˜ ì‹œì¥ ì„¸ì¤„ ìš”ì•½ (Market Color)
    * (ì˜ˆ: "ì§€ìˆ˜ëŠ” ë³´í•©ì´ë‚˜ 2ì°¨ì „ì§€ì™€ AI ë¡œë´‡ í…Œë§ˆê°€ í­ë°œí•˜ëŠ” ì¢…ëª© ì¥ì„¸")
    
    ### 2. ğŸ’° ìê¸ˆ íë¦„ ì¶”ì  (Money Flow)
    * **ëŒ€í˜•ì£¼:** ë°˜ë„ì²´, ë°”ì´ì˜¤, ê¸ˆìœµ ë“± ì‹œì´ ìƒìœ„ ì„¹í„°ì˜ ìˆ˜ê¸‰ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?
    * **ê°œë³„ì£¼:** ê¸‰ë“±ì£¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë³´ì´ëŠ” **'ì˜¤ëŠ˜ì˜ ê°•ì„¸ í…Œë§ˆ'**ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?
    
    ### 3. ğŸ“ˆ ì£¼ìš” ê±°ì‹œ ìš”ì¸ ë¶„ì„
    * í™˜ìœ¨, ê¸ˆë¦¬, ë¯¸ ì¦ì‹œ ì˜í–¥, ì •ë¶€ ì •ì±… ë“±ì´ ì˜¤ëŠ˜ ì‹œì¥ì— ë¯¸ì¹œ ì˜í–¥.
    
    ### 4. ğŸ’¼ íˆ¬ìì ëŒ€ì‘ ê°€ì´ë“œ
    * ì˜¤ëŠ˜ ê°™ì€ ì¥ì„¸ì—ì„œëŠ” **ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ íˆ¬ì**ê°€ ìœ ë¦¬í•©ë‹ˆê¹Œ? (ëŒíŒŒ ë§¤ë§¤ vs ëˆŒë¦¼ëª© ë§¤ìˆ˜ vs í˜„ê¸ˆ í™•ë³´)
    """
    
    try:
        response = model.generate_content(prompt, stream=True, safety_settings=safety_settings)
        for chunk in response:
            try:
                if chunk.text: yield chunk.text
            except ValueError: pass
    except Exception as e:
        yield f"âš ï¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ==========================================
# ğŸ–¥ï¸ ë©”ì¸ ì‹¤í–‰
# ==========================================
with st.sidebar:
    st.header("ğŸ” ì„¤ì •")
    if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.session_state.current_news_data = [] 
        st.rerun()
    
    if GOOG_API_KEY.startswith("AIza"):
        models = get_available_gemini_models(GOOG_API_KEY)
        
        # [Flash ëª¨ë¸ ê¸°ë³¸ ì„¤ì •]
        target_model = "gemini-flash-latest"
        default_index = 0
        if target_model in models:
            default_index = models.index(target_model)
        else:
            # flash ì´ë¦„ì´ ë“¤ì–´ê°„ ëª¨ë¸ ìë™ ì„ íƒ
            for i, m in enumerate(models):
                if "flash" in m.lower():
                    default_index = i
                    break
            
        model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", models, index=default_index)
        selected_real_name = model_name.split(" ")[1] if " " in model_name else model_name
    else:
        st.error("API í‚¤ í•„ìš”")
        selected_real_name = "gemini-1.5-flash"

# ì´ˆê¸° ë°ì´í„° ë¡œë”©
with st.status("ğŸš€ 3ì¤‘ í•„í„°(í…Œë§ˆ/ìƒìŠ¹/ê±°ë˜ëŒ€ê¸ˆ) ë°ì´í„° ìˆ˜ì§‘ ì¤‘...", expanded=True) as status:
    df_themes = get_top_50_themes_stocks() 
    riser_data = get_risers_data_with_market() 
    mf_codes = get_money_flow_codes()
    df_market_cap = get_market_cap_top150()
    df_kospi_gainers, df_kosdaq_gainers = get_top_gainers_df(limit=150)
    status.update(label="âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!", state="complete", expanded=False)

tab1, tab2 = st.tabs(["ğŸ¯ 3ì¤‘ êµì§‘í•© ë°œêµ´", "ğŸ“Š ì‹œí™© ë¶„ì„ (Dual-Engine)"])

# --- Tab 1 ---
with tab1:
    st.subheader("1ï¸âƒ£ 3ì¤‘ êµì§‘í•© ë¶„ì„ ê²°ê³¼ (The Intersection)")
    st.markdown("""
    **í•„í„°ë§ ì¡°ê±´ (AND ì¡°ê±´):**
    1. ğŸ”¥ **í…Œë§ˆ ìƒìœ„ 50ìœ„** ë‚´ ì¢…ëª©
    2. ğŸ“ˆ **ìƒìŠ¹ë¥  ìƒìœ„ 500ìœ„** (ì½”ìŠ¤í”¼+ì½”ìŠ¤ë‹¥)
    3. ğŸ’° **ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ 500ìœ„** (ì½”ìŠ¤í”¼+ì½”ìŠ¤ë‹¥)
    """)
    
    st.info(f"ğŸ“Š **ë°ì´í„° ìˆ˜ì§‘ í˜„í™©**")
    col1, col2, col3 = st.columns(3)
    col1.metric("ğŸ”¥ í…Œë§ˆ ì¢…ëª©", f"{len(df_themes)}ê°œ")
    col2.metric("ğŸ“ˆ ìƒìŠ¹ ì¢…ëª©", f"{len(riser_data)}ê°œ")
    col3.metric("ğŸ’° ê±°ë˜ëŒ€ê¸ˆ ì¢…ëª©", f"{len(mf_codes)}ê°œ")
    
    final_candidates = []
    
    if not df_themes.empty:
        for index, row in df_themes.iterrows():
            code = row['code']
            if (code in riser_data) and (code in mf_codes):
                row_data = row.to_dict()
                row_data['ì‹œì¥'] = riser_data[code] 
                final_candidates.append(row_data)
                
    if final_candidates:
        df_final = pd.DataFrame(final_candidates)
        df_final = df_final.drop_duplicates(['code'])
        df_final = df_final.sort_values(by="í…Œë§ˆìˆœìœ„_int")
        
        event = st.dataframe(
            df_final[['í…Œë§ˆìˆœìœ„', 'í…Œë§ˆë‚´ìˆœìœ„', 'ì‹œì¥', 'ì¢…ëª©ëª…', 'í˜„ì¬ê°€(ë“±ë½ë¥ )', 'í…Œë§ˆëª…']], 
            use_container_width=True, 
            hide_index=True, 
            on_select="rerun", 
            selection_mode="single-row",
            column_config={
                "í…Œë§ˆìˆœìœ„": st.column_config.TextColumn("í…Œë§ˆë­í‚¹", width="small"),
                "í…Œë§ˆë‚´ìˆœìœ„": st.column_config.TextColumn("í…Œë§ˆë‚´ë“±ìˆ˜", width="small"),
                "ì‹œì¥": st.column_config.TextColumn("ì‹œì¥", width="small"),
                "ì¢…ëª©ëª…": st.column_config.TextColumn("ì¢…ëª©ëª…", width="medium"),
                "í˜„ì¬ê°€(ë“±ë½ë¥ )": st.column_config.TextColumn("í˜„ì¬ê°€", width="medium"),
                "í…Œë§ˆëª…": st.column_config.TextColumn("í…Œë§ˆëª…", width="medium"),
            }
        )
        
        st.divider()
        
        if len(event.selection.rows) > 0:
            sel_idx = event.selection.rows[0]
            sel_data = df_final.iloc[sel_idx]
            
            s_name = sel_data['ì¢…ëª©ëª…']
            code = sel_data['code']
            s_theme = sel_data['í…Œë§ˆëª…']
            
            if st.session_state.last_code != code:
                st.session_state.messages = []
                st.session_state.last_code = code
                st.session_state.current_news_data = [] 
                
                with st.spinner(f"âš¡ {s_name} ì‹¤ì‹œê°„ ì†ë³´ ìˆ˜ì§‘ ì¤‘..."):
                    # [í•µì‹¬] ê²€ìƒ‰ì–´ ì ìš©: {ì¢…ëª©ëª…} ì£¼ê°€ íŠ¹ì§•ì£¼ / {ì¢…ëª©ëª…} ì†ë³´
                    news_1 = fetch_google_news_rss(f"{s_name} ì£¼ê°€ íŠ¹ì§•ì£¼", limit=20)
                    news_2 = fetch_google_news_rss(f"{s_name} ì†ë³´", limit=20)
                    
                    all_news = news_1 + news_2
                    unique_news = {v['link']: v for v in all_news}.values()
                    
                    sorted_news = sorted(list(unique_news), key=lambda x: x['date_obj'] if x['date_obj'] else datetime.min.replace(tzinfo=None), reverse=True)
                    st.session_state.current_news_data = sorted_news
                    
                    market_str = f"ì¢…ëª©ëª…: {s_name}\nì½”ë“œ: {code}\ní…Œë§ˆ: {s_theme}\nì‹œê°€ì´ì•¡: {get_stock_fundamentals(code)['ì‹œê°€ì´ì•¡']}\ní˜„ì¬ê°€(ë“±ë½): {sel_data['í˜„ì¬ê°€(ë“±ë½ë¥ )']}"
                    st.session_state.current_market_fact = market_str

            st.subheader(f"2ï¸âƒ£ [{s_name}] ìƒì„¸ ë¶„ì„")
            st.info(f"ğŸ’° ì‹œê°€ì´ì•¡: **{get_stock_fundamentals(code)['ì‹œê°€ì´ì•¡']}** | ğŸ† í…Œë§ˆ: **{s_theme}**")
            
            with st.expander("ğŸ’¬ AI íˆ¬ì ì „ëµê°€ì™€ ëŒ€í™”í•˜ê¸° (Click)", expanded=True):
                news_count = len(st.session_state.current_news_data)
                if news_count > 0:
                    st.success(f"âœ… **ë‰´ìŠ¤ {news_count}ê±´ í™•ë³´ë¨.**")
                else:
                    st.warning("âš ï¸ ìµœê·¼ 7ì¼ ë‚´ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

                for msg in st.session_state.messages:
                    if msg['role'] == 'user' and "ë‹¹ì‹ ì€" in msg['content']: continue
                    with st.chat_message(msg['role']): st.markdown(msg['content'])

                if not st.session_state.messages:
                    if st.button(f"âš¡ '{s_name}' ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"):
                        user_msg_content = f"{s_name} ì‹¬ì¸µ ë¶„ì„í•´ì¤˜."
                        with st.chat_message("user"): st.markdown(user_msg_content)
                        st.session_state.messages.append({"role": "user", "content": user_msg_content})
                        
                        with st.chat_message("assistant"):
                            res_txt = st.write_stream(get_gemini_response_stock_deep(
                                st.session_state.messages, 
                                selected_real_name, 
                                s_name, 
                                s_theme, 
                                st.session_state.current_market_fact, 
                                st.session_state.current_news_data
                            ))
                        st.session_state.messages.append({"role": "assistant", "content": res_txt})

                if prompt := st.chat_input(f"{s_name} ì§ˆë¬¸..."):
                    with st.chat_message("user"): st.markdown(prompt)
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    with st.chat_message("assistant"):
                        res_txt = st.write_stream(get_gemini_response_stock_deep(
                            st.session_state.messages, 
                            selected_real_name, 
                            s_name, 
                            s_theme, 
                            st.session_state.current_market_fact, 
                            st.session_state.current_news_data
                        ))
                    st.session_state.messages.append({"role": "assistant", "content": res_txt})

            col1, col2 = st.columns([1, 1])
            with col1:
                t1, t2, t3 = st.tabs(["ğŸ“… ì¼ë´‰", "ğŸ“† ì£¼ë´‰", "ğŸ“‹ í…Œë§ˆ ì „ì²´"])
                with t1: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/day/{code}.png", use_container_width=True)
                with t2: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/week/{code}.png", use_container_width=True)
                with t3:
                    cur_theme_list = df_themes[df_themes['í…Œë§ˆëª…']==s_theme]
                    st.dataframe(cur_theme_list[['í…Œë§ˆë‚´ìˆœìœ„', 'ì¢…ëª©ëª…','í˜„ì¬ê°€(ë“±ë½ë¥ )']], hide_index=True)
            with col2:
                final_news_list = st.session_state.current_news_data
                st.markdown(f"##### ğŸ“° ê´€ë ¨ ë‰´ìŠ¤ (ìƒìœ„ 20ê±´)")
                if final_news_list:
                    for n in final_news_list[:20]: 
                        st.markdown(f"- [{n['title']}]({n['link']})")
                else:
                    st.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ì¡°ê±´(í…Œë§ˆ50ìœ„ & ìƒìŠ¹500ìœ„ & ê±°ë˜ëŒ€ê¸ˆ500ìœ„)ì„ ë™ì‹œì— ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ í˜„ì¬ ì—†ìŠµë‹ˆë‹¤.")

# --- Tab 2 ---
with tab2:
    st.header("ğŸ“Š ì‹œì¥ ì…ì²´ ë¶„ì„ (ëŒ€í˜•ì£¼ vs ì£¼ë„ì£¼)")
    sub_t1, sub_t2 = st.tabs(["ğŸ¢ ì‹œì´ ìƒìœ„ 150 (ì§€ìˆ˜)", "ğŸš€ ê¸‰ë“± ìƒìœ„ 150 (ëª¨ë©˜í…€)"])
    with sub_t1:
        if not df_market_cap.empty: st.dataframe(df_market_cap, height=400, use_container_width=True)
    with sub_t2:
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("#### ì½”ìŠ¤í”¼ ê¸‰ë“± Top 150")
            if not df_kospi_gainers.empty: st.dataframe(df_kospi_gainers, height=400, use_container_width=True)
        with c2:
            st.markdown("#### ì½”ìŠ¤ë‹¥ ê¸‰ë“± Top 150")
            if not df_kosdaq_gainers.empty: st.dataframe(df_kosdaq_gainers, height=400, use_container_width=True)
        
    st.divider()
    st.subheader("ğŸ¤– AI ì‹¤ì‹œê°„ ì‹œí™© ë¸Œë¦¬í•‘")
    if st.button("ğŸ“¢ ì‹œí™© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¢…í•© ë¶„ì„ (RSS)"):
        with st.spinner("ì‹¤ì‹œê°„ ì¦ì‹œ ì†ë³´ ìˆ˜ì§‘ ì¤‘..."):
            # [í•µì‹¬] ì‹œí™© ê²€ìƒ‰ì–´: êµ­ë‚´ ì¦ì‹œ / í•œêµ­ ì¦ì‹œ
            news_1 = fetch_google_news_rss("êµ­ë‚´ ì¦ì‹œ", limit=20)
            news_2 = fetch_google_news_rss("í•œêµ­ ì¦ì‹œ", limit=20)
            
            all_market_news = news_1 + news_2
            unique_market_news = {v['link']: v for v in all_market_news}.values()
            
            final_market_news = sorted(list(unique_market_news), key=lambda x: x['date_obj'] if x['date_obj'] else datetime.min.replace(tzinfo=None), reverse=True)
            
        if final_market_news:
            st.success(f"âœ… ë‰´ìŠ¤ {len(final_market_news)}ê±´ í™•ë³´! (ë¶„ì„ ì‹œì‘)")
            with st.expander("ğŸ” ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° í™•ì¸", expanded=False):
                for n in final_market_news:
                    st.write(f"- {n['title']}: {n['summary']}")
            st.write_stream(analyze_market_macro_v2(df_market_cap, df_kospi_gainers, df_kosdaq_gainers, final_market_news, selected_real_name))
        else:
            st.error("âš ï¸ ìµœê·¼ 7ì¼ ë‚´ ì‹œí™© ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
