import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import google.generativeai as genai
import urllib.parse

# ==========================================
# ğŸ”‘ [í•„ìˆ˜] Gemini API í‚¤ ì„¤ì •
# ==========================================
try:
    GOOG_API_KEY = st.secrets["GOOG_API_KEY"]
except:
    GOOG_API_KEY = "ì—¬ê¸°ì—_í‚¤ë¥¼_ë„£ìœ¼ì„¸ìš”"

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì£¼ì‹ í…Œë§ˆ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ¤– AI ì£¼ì‹ íˆ¬ì ì „ëµê°€77 (Google RSS Mass Analysis Ver.)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_code" not in st.session_state:
    st.session_state.last_code = None

# --- [ëª¨ë¸ ëª©ë¡] ---
@st.cache_data
def get_available_gemini_models(api_key):
    try:
        genai.configure(api_key=api_key)
        return [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except: return ["gemini-1.5-flash"]

# --- [í•µì‹¬] Google News RSS ìˆ˜ì§‘ê¸° (ë³¸ë¬¸ ëŒ€ì²´ìš© ìš”ì•½ ë°ì´í„° í™•ë³´) ---
def fetch_google_news_rss(keyword, limit=30):
    """
    êµ¬ê¸€ ë‰´ìŠ¤ RSSë¥¼ í†µí•´ ì œëª©, ë§í¬, ë‚ ì§œ, ê·¸ë¦¬ê³  'ìš”ì•½(Description)'ì„ ëŒ€ëŸ‰ìœ¼ë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    ì´ ë°©ì‹ì€ ì°¨ë‹¨ë˜ì§€ ì•Šìœ¼ë©°, ë³¸ë¬¸ì„ ì½ì§€ ì•Šì•„ë„ ê¸°ì‚¬ì˜ í•µì‹¬ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    news_data = []
    try:
        # RSS URL ìƒì„± (í•œêµ­ì–´, í•œêµ­ ì§€ì—­)
        encoded_kw = urllib.parse.quote(keyword)
        url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=ko&gl=KR&ceid=KR:ko"
        
        # í—¤ë” ì—†ì´ë„ RSSëŠ” ì˜ ë™ì‘í•˜ì§€ë§Œ, ê¸°ë³¸ í—¤ë” ì¶”ê°€
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        
        if res.status_code == 200:
            # XML íŒŒì‹±
            soup = BeautifulSoup(res.content, 'xml')
            items = soup.find_all('item')
            
            for item in items[:limit]:
                title = item.title.text
                link = item.link.text
                pub_date = item.pubDate.text
                
                # description íƒœê·¸ ì•ˆì— ìš”ì•½ë¬¸ì´ ë“¤ì–´ìˆìŒ (HTML íƒœê·¸ í¬í•¨ë  ìˆ˜ ìˆìŒ)
                raw_desc = item.description.text
                # HTML íƒœê·¸ ì œê±° ë° ì •ì œ
                clean_desc = BeautifulSoup(raw_desc, "html.parser").get_text(separator=" ", strip=True)
                
                # ì¶œì²˜ ì¶”ì¶œ (ì œëª© ë’¤ ' - ë§¤ì²´ëª…')
                source = "News"
                if "-" in title:
                    source = title.split("-")[-1].strip()
                    
                news_data.append({
                    "source": source,
                    "title": title,
                    "link": link,
                    "summary": clean_desc, # ì´ê²Œ ë³¸ë¬¸ ì—­í• ì„ ëŒ€ì‹ í•¨
                    "date": pub_date
                })
    except Exception as e:
        print(f"RSS Error: {e}")
        
    return news_data

# --- [ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤ (ë„¤ì´ë²„ ê¸ˆìœµ í…Œì´ë¸”)] ---
@st.cache_data
def get_naver_themes():
    url = "https://finance.naver.com/sise/theme.naver"
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        data = []
        for row in soup.select("#contentarea_left > table.type_1 > tr"):
            cols = row.select("td")
            if len(cols) >= 4:
                data.append({"í…Œë§ˆëª…": cols[0].text.strip(), "ë§í¬": "https://finance.naver.com" + cols[0].find('a')['href']})
        return pd.DataFrame(data).head(20)
    except: return pd.DataFrame()

def get_theme_details(theme_link):
    try:
        res = requests.get(theme_link, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        stocks = []
        for row in soup.select("table.type_5 > tbody > tr"):
            cols = row.select("td")
            if len(cols) > 4:
                name = cols[0].text.strip()
                link = "https://finance.naver.com" + cols[0].find('a')['href']
                code_match = re.search(r'code=([0-9]+)', link)
                code = code_match.group(1) if code_match else ""
                price = cols[2].text.strip()
                rate = cols[4].text.strip().replace('\n', '').strip()
                stocks.append({'name': name, 'code': code, 'price_str': f"{price} ({rate})", 'link': theme_link})
        return stocks
    except: return []

@st.cache_data
def get_all_theme_stocks():
    df_themes = get_naver_themes()
    all_stocks = []
    for index, row in df_themes.iterrows():
        stocks_info = get_theme_details(row['ë§í¬'])
        stocks_info.sort(key=lambda x: float(x['price_str'].split('(')[1].replace('%)','').replace('+','').replace('-','-').replace(',','')) if '(' in x['price_str'] else 0, reverse=True)
        for rank, stock in enumerate(stocks_info, 1):
             all_stocks.append({
                 "í…Œë§ˆìˆœìœ„": f"{rank}ìœ„", "ì¢…ëª©ëª…": stock['name'], "ì¢…ëª©ì½”ë“œ": stock['code'], 
                 "í…Œë§ˆëª…": row['í…Œë§ˆëª…'], "í˜„ì¬ê°€(ë“±ë½ë¥ )": stock['price_str']
             })
    return pd.DataFrame(all_stocks)

@st.cache_data
def get_top_risers_info():
    market_map = {}
    for s in [0, 1]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_rise.naver?sosok={s}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for item in soup.select("table.type_2 tr td a.tltle")[:300]: 
                market_map[item.text.strip()] = "KOSPI" if s==0 else "KOSDAQ"
        except: pass
    return market_map

@st.cache_data
def get_volume_leaders():
    tickers = []
    for s in [0, 1]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_quant_high.naver?sosok={s}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.text, 'html.parser')
            for item in soup.select("table.type_2 tr td a.tltle")[:200]: 
                tickers.append(item.text.strip())
        except: pass
    return tickers

def get_stock_fundamentals(code):
    try:
        url = f"https://finance.naver.com/item/main.naver?code={code}"
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        cap_elem = soup.select_one("#_market_sum")
        if cap_elem:
            raw_cap = cap_elem.text.strip()
            raw_cap = re.sub(r'[è­°å…†]', 'ì¡°', raw_cap)
            raw_cap = raw_cap.replace('\t', '').replace('\n', '').replace('  ', ' ') + "ì–µ"
            return {"ì‹œê°€ì´ì•¡": raw_cap}
    except: pass
    return {"ì‹œê°€ì´ì•¡": "-"}

@st.cache_data
def get_market_cap_top150():
    stocks = []
    for page in range(1, 4):
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page={page}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for row in soup.select("table.type_2 tbody tr"):
                cols = row.select("td")
                if len(cols) < 10: continue
                stocks.append({
                    "ìˆœìœ„": cols[0].text.strip(), "ì¢…ëª©ëª…": cols[1].text.strip(),
                    "í˜„ì¬ê°€": cols[2].text.strip(), "ë“±ë½ë¥ ": cols[4].text.strip().replace("\n", "").strip(),
                    "ì‹œê°€ì´ì•¡": cols[6].text.strip()
                })
        except: pass
    return pd.DataFrame(stocks)

# --- [AI ì‘ë‹µ í•¨ìˆ˜] ---
def get_gemini_response_mass_analysis(messages, model_name, stock_name, theme, market_data_str, news_data):
    genai.configure(api_key=GOOG_API_KEY)
    
    current_query = messages[-1]['content']
    search_res = ""
    
    if "ë‹¹ì‹ ì€" in current_query:
        # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ê±°ëŒ€í•œ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¡œ ë³€í™˜ (ìš”ì•½ë¬¸ ì¤‘ì‹¬)
        combined_news_context = ""
        for i, item in enumerate(news_data):
            # [ì¶œì²˜] ì œëª© (ë‚ ì§œ) \n ìš”ì•½ë¬¸
            combined_news_context += f"[{i+1}. {item['source']}] {item['title']} ({item['date']})\n> ìš”ì•½: {item['summary']}\n\n"
            
        search_res = f"""
        \n[ì‹œìŠ¤í…œ ë°ì´í„° ì£¼ì…]
        1. ğŸ“Š ì‹œì¥ ë°ì´í„° (Hard Fact):
        {market_data_str}
        
        2. ğŸ“° ë‰´ìŠ¤ ëŒ€ëŸ‰ ìš”ì•½ ë°ì´í„° (Soft Fact - {len(news_data)}ê±´):
        {combined_news_context}
        """
    
    modified_msgs = []
    for i, msg in enumerate(messages):
        content = msg['content']
        if i == len(messages)-1: content += search_res
        modified_msgs.append({"role": "user" if msg['role']=="user" else "model", "parts": [content]})
    
    model = genai.GenerativeModel(f"models/{model_name}")
    
    try:
        response = model.generate_content(modified_msgs, stream=True)
        for chunk in response: yield chunk.text
    except Exception as e:
        yield f"âš ï¸ API ì˜¤ë¥˜: {str(e)}\n\n(API í‚¤ í•œë„ê°€ ì´ˆê³¼ë˜ì—ˆê±°ë‚˜ ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì…ë‹ˆë‹¤.)"

# ==========================================
# ğŸ–¥ï¸ ë©”ì¸ ì‹¤í–‰
# ==========================================
with st.sidebar:
    st.header("ğŸ” ì„¤ì •")
    if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.rerun()
    
    if GOOG_API_KEY.startswith("AIza"):
        models = get_available_gemini_models(GOOG_API_KEY)
        model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", models, index=0)
        selected_real_name = model_name.split(" ")[1] if " " in model_name else model_name
    else:
        st.error("API í‚¤ í•„ìš”")
        selected_real_name = "gemini-1.5-flash"

# ì´ˆê¸° ë°ì´í„° ë¡œë”©
with st.status("ğŸš€ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (Naver & Google RSS)", expanded=True) as status:
    df_market = get_market_cap_top150()
    market_map = get_top_risers_info()
    vol_leaders = get_volume_leaders()
    df_C = get_all_theme_stocks()
    status.update(label="âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!", state="complete", expanded=False)

tab1, tab2 = st.tabs(["ğŸ¯ ê¸‰ë“±ì£¼ ë°œêµ´", "ğŸ“Š ì‹œí™© ë¶„ì„"])

# --- Tab 1 ---
with tab1:
    st.subheader("1ï¸âƒ£ êµì§‘í•© ë¶„ì„ ê²°ê³¼ (í•µì‹¬ ì£¼ë„ì£¼)")
    list_A = list(market_map.keys())
    list_B = vol_leaders
    final_candidates = []
    
    for index, row in df_C.iterrows():
        stock_name = row['ì¢…ëª©ëª…']
        if (stock_name in list_A) and (stock_name in list_B):
            market_type = market_map.get(stock_name, "Unknown")
            row_data = row.to_dict()
            row_data['ì‹œì¥êµ¬ë¶„'] = market_type
            final_candidates.append(row_data)
            
    if final_candidates:
        df_final = pd.DataFrame(final_candidates)
        df_final = df_final.drop_duplicates(['ì¢…ëª©ëª…'])
        df_final = df_final.sort_values(by="í…Œë§ˆëª…")
        
        event = st.dataframe(
            df_final[['í…Œë§ˆìˆœìœ„', 'ì‹œì¥êµ¬ë¶„', 'ì¢…ëª©ëª…', 'í˜„ì¬ê°€(ë“±ë½ë¥ )', 'í…Œë§ˆëª…']], 
            use_container_width=True, hide_index=True, on_select="rerun", selection_mode="single-row"
        )
        
        st.divider()
        
        if len(event.selection.rows) > 0:
            sel_idx = event.selection.rows[0]
            sel_data = df_final.iloc[sel_idx]
            s_name = sel_data['ì¢…ëª©ëª…']
            code = sel_data['ì¢…ëª©ì½”ë“œ']
            s_theme = sel_data['í…Œë§ˆëª…']
            
            if st.session_state.last_code != code:
                st.session_state.messages = []
                st.session_state.last_code = code

            # [RSS ìˆ˜ì§‘] ì¢…ëª© ë‰´ìŠ¤ + í˜¸ì¬ ê²€ìƒ‰ (ì´ 50ê°œ ëª©í‘œ)
            with st.spinner(f"ğŸ” {s_name} ê´€ë ¨ ë‰´ìŠ¤ ë°ì´í„° 50ê±´ ìˆ˜ì§‘ ì¤‘..."):
                fund = get_stock_fundamentals(code)
                # Google RSSëŠ” ì°¨ë‹¨ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì•ˆì •ì 
                news_1 = fetch_google_news_rss(f"{s_name} ì£¼ê°€", limit=25)
                news_2 = fetch_google_news_rss(f"{s_name} í˜¸ì¬ íŠ¹ì§•ì£¼", limit=25)
                
                # ì¤‘ë³µ ì œê±° (ë§í¬ ê¸°ì¤€)
                all_news = news_1 + news_2
                unique_news = {v['link']: v for v in all_news}.values()
                final_news_list = list(unique_news)
                
                # ì‹œì¥ ë°ì´í„° ë¬¸ìì—´ ìƒì„± (AI ì£¼ì…ìš©)
                market_data_str = f"ì¢…ëª©ëª…: {s_name}\nì½”ë“œ: {code}\ní…Œë§ˆ: {s_theme}\nì‹œê°€ì´ì•¡: {fund['ì‹œê°€ì´ì•¡']}\ní˜„ì¬ê°€(ë“±ë½): {sel_data['í˜„ì¬ê°€(ë“±ë½ë¥ )']}\nì‹œì¥êµ¬ë¶„: {sel_data['ì‹œì¥êµ¬ë¶„']}"
            
            st.subheader(f"2ï¸âƒ£ [{s_name}] ìƒì„¸ ë¶„ì„")
            st.info(f"ğŸ’° ì‹œê°€ì´ì•¡: **{fund['ì‹œê°€ì´ì•¡']}** | ğŸ† í…Œë§ˆ: **{s_theme}**")
            
            with st.expander("ğŸ’¬ AI íˆ¬ì ì „ëµê°€ì™€ ëŒ€í™”í•˜ê¸° (Click)", expanded=True):
                if not st.session_state.messages:
                    if st.button(f"âš¡ '{s_name}' ì‹¬ì¸µ ë¶„ì„ ì‹œì‘ (ìš”ì•½ë¬¸ {len(final_news_list)}ê±´ ê¸°ë°˜)"):
                        
                        sys_prompt = f"""
                        ë‹¹ì‹ ì€ ì›”ê°€ ì¶œì‹ ì˜ í€€íŠ¸ ë° íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤.
                        ì œê³µëœ [ì‹œì¥ ë°ì´í„°(30% ë¹„ì¤‘)]ì™€ [ë‰´ìŠ¤ ìš”ì•½ ë°ì´í„°(70% ë¹„ì¤‘)]ë¥¼ ì¢…í•©í•˜ì—¬ ë¶„ì„í•˜ì‹­ì‹œì˜¤.
                        
                        [ë¶„ì„ ëª©í‘œ]
                        ë‰´ìŠ¤ ìš”ì•½ë¬¸ë“¤ì—ì„œ ë°˜ë³µë˜ëŠ” í‚¤ì›Œë“œì™€ íŒ©íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ìƒìŠ¹/í•˜ë½ì˜ 'ì§„ì§œ ì´ìœ 'ë¥¼ ì°¾ì•„ë‚´ê³ ,
                        34ì„¸ ì§ì¥ì¸ íˆ¬ììì—ê²Œ ë§ëŠ” ë§¤ë§¤ ì „ëµì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
                        
                        ë°˜ë“œì‹œ ë‹¤ìŒ í¬ë§·ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:
                        1. ğŸš€ í•µì‹¬ í˜¸ì¬/ì•…ì¬ 3ê°€ì§€ (íŒ©íŠ¸ ê¸°ë°˜)
                        2. ğŸ” ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„ (ì–¸ë¡ ì´ ì£¼ëª©í•˜ëŠ” í¬ì¸íŠ¸)
                        3. ğŸ’¡ ì‹¤ì „ ë§¤ë§¤ ì „ëµ (ë§¤ìˆ˜/ë§¤ë„/ê´€ë§ ë° ëª©í‘œê°€)
                        """
                        st.session_state.messages.append({"role": "user", "content": sys_prompt})
                        with st.chat_message("assistant"):
                            res_txt = st.write_stream(get_gemini_response_mass_analysis(st.session_state.messages, selected_real_name, s_name, s_theme, market_data_str, final_news_list))
                        st.session_state.messages.append({"role": "assistant", "content": res_txt})

                for msg in st.session_state.messages:
                    if msg['role'] == 'user' and "ë‹¹ì‹ ì€" in msg['content']: continue
                    with st.chat_message(msg['role']): st.markdown(msg['content'])

                if prompt := st.chat_input(f"{s_name} ì§ˆë¬¸..."):
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"): st.markdown(prompt)
                    with st.chat_message("assistant"):
                        # ëŒ€í™” ì‹œì—ëŠ” history ì „ë‹¬
                        model = genai.GenerativeModel(f"models/{selected_real_name}")
                        history = [{"role": "user" if m["role"]=="user" else "model", "parts": [m["content"]]} for m in st.session_state.messages]
                        try:
                            res = model.generate_content(history, stream=True)
                            res_txt = st.write_stream(res)
                        except Exception as e:
                            res_txt = f"âš ï¸ ì˜¤ë¥˜: {str(e)}"
                            st.write(res_txt)
                        st.session_state.messages.append({"role": "assistant", "content": res_txt})

            col1, col2 = st.columns([1, 1])
            with col1:
                t1, t2, t3 = st.tabs(["ğŸ“… ì¼ë´‰", "ğŸ“† ì£¼ë´‰", "ğŸ“‹ í…Œë§ˆ ì „ì²´"])
                with t1: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/day/{code}.png", use_container_width=True)
                with t2: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/week/{code}.png", use_container_width=True)
                with t3:
                    cur_theme_list = df_C[df_C['í…Œë§ˆëª…']==s_theme]
                    st.dataframe(cur_theme_list[['í…Œë§ˆìˆœìœ„','ì¢…ëª©ëª…','í˜„ì¬ê°€(ë“±ë½ë¥ )']], hide_index=True)
            with col2:
                st.markdown(f"##### ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ìš”ì•½ ({len(final_news_list)}ê±´)")
                st.caption("â€» Google News RSSì˜ ìš”ì•½ë¬¸(Description)ì„ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
                if final_news_list:
                    for n in final_news_list:
                        # ìš”ì•½ë¬¸ í‘œì‹œ
                        st.markdown(f"**[{n['title']}]({n['link']})**")
                        st.caption(f"{n['summary']}...") # RSSì—ì„œ ê°€ì ¸ì˜¨ Clean Description
                        st.divider()
                else:
                    st.warning("ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    else:
        st.warning("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- Tab 2 ---
with tab2:
    st.header("ğŸ“Š ì‹œì¥ ì „ì²´ íë¦„ (ì‹œì´ Top 150)")
    if df_market is not None:
        st.dataframe(df_market, height=400)
        
        st.subheader("ğŸ¤– AI ì‹¤ì‹œê°„ ì‹œí™© ë¸Œë¦¬í•‘")
        if st.button("ğŸ“¢ ì‹œí™© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ (RSS)"):
            with st.spinner("Google RSSì—ì„œ ì‹œí™© ë‰´ìŠ¤ 40ê±´ ìˆ˜ì§‘ ì¤‘..."):
                news_1 = fetch_google_news_rss("í•œêµ­ ì¦ì‹œ ì‹œí™©", limit=20)
                news_2 = fetch_google_news_rss("ì½”ìŠ¤í”¼ ì½”ìŠ¤ë‹¥ íŠ¹ì§•ì£¼", limit=20)
                
                all_market_news = news_1 + news_2
                unique_market_news = {v['link']: v for v in all_market_news}.values()
                final_market_news = list(unique_market_news)
                
                # ì‹œì¥ ë°ì´í„° ìš”ì•½
                top_30_str = df_market.head(30).to_string(index=False)
            
            if final_market_news:
                st.success(f"âœ… ë‰´ìŠ¤ {len(final_market_news)}ê±´ í™•ë³´! ë¶„ì„ ì‹œì‘.")
                with st.expander("ğŸ” ìˆ˜ì§‘ëœ ë°ì´í„° í™•ì¸", expanded=False):
                    for n in final_market_news:
                        st.write(f"- {n['title']}: {n['summary']}")
                
                st.write_stream(get_gemini_response_mass_analysis(
                    [{"role": "user", "content": "ë‹¹ì‹ ì€ ìˆ˜ì„ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì‹œí™©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."}], 
                    selected_real_name, 
                    "KOSPI/KOSDAQ", 
                    "Market", 
                    top_30_str, 
                    final_market_news
                ))
            else:
                st.error("âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨.")
