import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import google.generativeai as genai
import urllib.parse
import random

# ==========================================
# ğŸ”‘ [í•„ìˆ˜] Gemini API í‚¤ ì„¤ì •
# ==========================================
try:
    GOOG_API_KEY = st.secrets["GOOG_API_KEY"]
except:
    GOOG_API_KEY = "ì—¬ê¸°ì—_í‚¤ë¥¼_ë„£ìœ¼ì„¸ìš”"

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì£¼ì‹ í…Œë§ˆ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ¤– AI ì£¼ì‹ íˆ¬ì ì „ëµê°€4")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_code" not in st.session_state:
    st.session_state.last_code = None

# --- [ëª¨ë¸ ëª©ë¡] ---
@st.cache_data
def get_available_gemini_models(api_key):
    try:
        genai.configure(api_key=api_key)
        return [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except: return ["gemini-1.5-flash"]

# --- [í•µì‹¬] Google News RSS ìˆ˜ì§‘ê¸° ---
def fetch_google_news_rss(keyword, limit=30):
    news_data = []
    try:
        encoded_kw = urllib.parse.quote(keyword)
        url = f"https://news.google.com/rss/search?q={encoded_kw}&hl=ko&gl=KR&ceid=KR:ko"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers, timeout=5)
        
        if res.status_code == 200:
            soup = BeautifulSoup(res.content, 'xml')
            items = soup.find_all('item')
            
            for item in items[:limit]:
                title = item.title.text
                link = item.link.text
                pub_date = item.pubDate.text
                
                raw_desc = item.description.text
                clean_desc = BeautifulSoup(raw_desc, "html.parser").get_text(separator=" ", strip=True)
                
                source = "News"
                if "-" in title:
                    parts = title.rsplit("-", 1)
                    if len(parts) > 1:
                        source = parts[1].strip()
                        title = parts[0].strip()
                    
                news_data.append({
                    "source": source,
                    "title": title,
                    "link": link,
                    "summary": clean_desc,
                    "date": pub_date
                })
    except Exception as e:
        print(f"RSS Error: {e}")
    return news_data

# --- [ë°ì´í„° ìˆ˜ì§‘ 1: í…Œë§ˆ ìƒìœ„ 50ê°œ ë° ì†Œì† ì¢…ëª©] ---
@st.cache_data
def get_top_50_themes_stocks():
    url = "https://finance.naver.com/sise/theme.naver"
    all_theme_stocks = [] 
    
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        
        theme_links = []
        for row in soup.select("#contentarea_left > table.type_1 > tr"):
            cols = row.select("td")
            if len(cols) >= 4:
                theme_name = cols[0].text.strip()
                link = "https://finance.naver.com" + cols[0].find('a')['href']
                theme_links.append({"name": theme_name, "link": link})
                if len(theme_links) >= 50: break
        
        progress_bar = st.progress(0)
        for idx, theme in enumerate(theme_links):
            try:
                res_t = requests.get(theme['link'], headers={'User-Agent': 'Mozilla/5.0'})
                soup_t = BeautifulSoup(res_t.content.decode('cp949', 'ignore'), 'html.parser')
                
                for row in soup_t.select("table.type_5 > tbody > tr"):
                    cols = row.select("td")
                    if len(cols) > 4:
                        name_tag = cols[0].find('a')
                        if not name_tag: continue
                        
                        stock_name = name_tag.text.strip()
                        link_sub = name_tag['href']
                        code_match = re.search(r'code=([0-9]+)', link_sub)
                        code = code_match.group(1) if code_match else ""
                        
                        price_str = cols[2].text.strip() + " (" + cols[4].text.strip().replace('\n', '').strip() + ")"
                        
                        all_theme_stocks.append({
                            "code": code, 
                            "ì¢…ëª©ëª…": stock_name,
                            "í…Œë§ˆëª…": theme['name'],
                            "í…Œë§ˆìˆœìœ„": f"{idx+1}ìœ„",
                            "í˜„ì¬ê°€(ë“±ë½ë¥ )": price_str
                        })
            except: pass
            progress_bar.progress((idx + 1) / len(theme_links))
        progress_bar.empty()
        
    except: pass
    return pd.DataFrame(all_theme_stocks)

# --- [ë°ì´í„° ìˆ˜ì§‘ 2: ìƒìŠ¹ë¥  ìƒìœ„ ì¢…ëª© (ì½”ë“œë§Œ - êµì§‘í•©ìš©)] ---
@st.cache_data
def get_risers_codes_only():
    riser_codes = set()
    for s in [0, 1]: 
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_rise.naver?sosok={s}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            count = 0
            for item in soup.select("table.type_2 tr td a.tltle"):
                if count >= 500: break
                link = item['href']
                code_match = re.search(r'code=([0-9]+)', link)
                if code_match:
                    riser_codes.add(code_match.group(1))
                    count += 1
        except: pass
    return riser_codes

# --- [ë°ì´í„° ìˆ˜ì§‘ 2-1: ìƒìŠ¹ë¥  ìƒìœ„ ì¢…ëª© (ìƒì„¸ ì •ë³´ - ì‹œí™© ë¶„ì„ìš©)] ---
# [New] ì‹œí™© ë¶„ì„ íƒ­ì— ë³´ì—¬ì¤„ ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ê°ê° 150ê°œ ìƒì„¸ ë°ì´í„°
@st.cache_data
def get_top_gainers_df(limit=150):
    kospi_gainers = []
    kosdaq_gainers = []
    
    # 0: ì½”ìŠ¤í”¼, 1: ì½”ìŠ¤ë‹¥
    for market_code, result_list in [(0, kospi_gainers), (1, kosdaq_gainers)]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_rise.naver?sosok={market_code}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            
            rows = soup.select("table.type_2 tr")
            count = 0
            for row in rows:
                cols = row.select("td")
                if len(cols) < 5: continue # í—¤ë”ë‚˜ ë¹ˆ ì¤„ ìŠ¤í‚µ
                
                # ì¢…ëª©ëª… íƒœê·¸ ì°¾ê¸°
                name_tag = cols[1].find('a')
                if not name_tag: continue
                
                name = name_tag.text.strip()
                price = cols[2].text.strip()
                rate = cols[4].text.strip().replace('\n', '').strip()
                
                result_list.append({
                    "ì¢…ëª©ëª…": name,
                    "í˜„ì¬ê°€": price,
                    "ë“±ë½ë¥ ": rate
                })
                count += 1
                if count >= limit: break
        except: pass
        
    return pd.DataFrame(kospi_gainers), pd.DataFrame(kosdaq_gainers)

# --- [ë°ì´í„° ìˆ˜ì§‘ 3: ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ ì¢…ëª©] ---
@st.cache_data
def get_money_flow_codes():
    mf_codes = set()
    headers = {'User-Agent': 'Mozilla/5.0'}
    for s in [0, 1]:
        for page in range(1, 6):
            try:
                url = f"https://finance.naver.com/sise/sise_market_sum.naver?sosok={s}&sort=amount&page={page}"
                res = requests.get(url, headers=headers)
                soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
                items = soup.select("table.type_2 tbody tr td:nth-child(2) a")
                for item in items:
                    link = item['href']
                    code_match = re.search(r'code=([0-9]+)', link)
                    if code_match:
                        mf_codes.add(code_match.group(1))
            except: pass
            time.sleep(0.1)
    return mf_codes

# --- [ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°] ---
def get_stock_fundamentals(code):
    try:
        url = f"https://finance.naver.com/item/main.naver?code={code}"
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        cap_elem = soup.select_one("#_market_sum")
        if cap_elem:
            raw_cap = cap_elem.text.strip()
            raw_cap = re.sub(r'[è­°å…†]', 'ì¡°', raw_cap)
            raw_cap = raw_cap.replace('\t', '').replace('\n', '').replace('  ', ' ') + "ì–µ"
            return {"ì‹œê°€ì´ì•¡": raw_cap}
    except: pass
    return {"ì‹œê°€ì´ì•¡": "-"}

@st.cache_data
def get_market_cap_top150():
    stocks = []
    for page in range(1, 4):
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page={page}", headers={'User-Agent': 'Mozilla/5.0'})
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for row in soup.select("table.type_2 tbody tr"):
                cols = row.select("td")
                if len(cols) < 10: continue
                stocks.append({
                    "ìˆœìœ„": cols[0].text.strip(), "ì¢…ëª©ëª…": cols[1].text.strip(),
                    "í˜„ì¬ê°€": cols[2].text.strip(), "ë“±ë½ë¥ ": cols[4].text.strip().replace("\n", "").strip(),
                    "ì‹œê°€ì´ì•¡": cols[6].text.strip()
                })
        except: pass
    return pd.DataFrame(stocks)

# --- [AI ì‘ë‹µ í•¨ìˆ˜ 1: ê°œë³„ ì¢…ëª© (World-Class)] ---
def get_gemini_response_stock_deep(messages, model_name, stock_name, theme, market_data_str, news_data):
    genai.configure(api_key=GOOG_API_KEY)
    
    current_query = messages[-1]['content']
    search_res = ""
    
    if "ë‹¹ì‹ ì€" in current_query:
        combined_news_context = ""
        for i, item in enumerate(news_data):
            combined_news_context += f"[{i+1}. {item['source']}] {item['title']} ({item['date']})\n> ìš”ì•½: {item['summary']}\n\n"
            
        search_res = f"""
        \n[ë¶„ì„ ëŒ€ìƒ ë°ì´í„°]
        1. ğŸ“Š ì •ëŸ‰ì  ë°ì´í„° (Market Fact):
        {market_data_str}
        
        2. ğŸ“° ì •ì„±ì  ë°ì´í„° (News Buzz - ì´ {len(news_data)}ê±´):
        {combined_news_context}
        """
        
        sys_instructions = """
        ë‹¹ì‹ ì€ ëƒ‰ì² í•œ íŒë‹¨ë ¥ì„ ê°€ì§„ ì„¸ê³„ìµœê³  ì£¼ì‹ ì• ë„ë¦¬ìŠ¤íŠ¸ ê²¸ ë¶„ì„ê°€ ì…ë‹ˆë‹¤.
        
        ì œê³µëœ [ì •ëŸ‰ ë°ì´í„°]ì™€ [ë‰´ìŠ¤ ë°ì´í„°]ë¥¼ êµì°¨ ê²€ì¦í•˜ì—¬ ë‹¤ìŒ êµ¬ì¡°ë¡œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

        ê¸´ë§í•˜ì§€ë§ê³  ë°”ë¡œ ë¶„ì„ì— ë“¤ì–´ê°€ ì£¼ì„¸ìš”.
        
        ### 1. ğŸ¯ AI íˆ¬ì ë§¤ë ¥ë„ ì ìˆ˜ (100ì  ë§Œì )
        * **ì ìˆ˜:** OOOì 
        * **í•œì¤„ í‰:** (ì˜ˆ: "ê°•ë ¥í•œ í˜¸ì¬ì™€ ìˆ˜ê¸‰ì´ ë§Œë‚œ ìƒìŠ¹ ì´ˆì… êµ¬ê°„ì…ë‹ˆë‹¤" ë˜ëŠ” "ì¬ë£Œ ì†Œë©¸ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”")
        
        ### 2. ğŸš€ í•µì‹¬ ìƒìŠ¹ ë™ë ¥ (Momentum & Catalyst) ë° í˜¸ì¬ë¶„ì„
        * ë‰´ìŠ¤ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” **'ì§„ì§œ í˜¸ì¬(Fact)'** 3ê°€ì§€ë¥¼ íŒ©íŠ¸ ìœ„ì£¼ë¡œ ìš”ì•½í•˜ì„¸ìš”.(ë‰´ìŠ¤ë¥¼ ì–¸ê¸‰í•  í•„ìš”ëŠ” ì—†ìŠµë‹ˆë‹¤. ì£¼ì œíŒŒì•…ë§Œ í•˜ì„¸ìš”)
        * ë‹¨ìˆœ ê¸°ëŒ€ê°ì¸ì§€, ì‹¤ì§ˆì ì¸ ìˆ˜ì£¼/ì‹¤ì /ì •ì±… ìˆ˜í˜œì¸ì§€ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.
        
        ### 3. âš ï¸ ë¦¬ìŠ¤í¬ ë° ìˆ˜ê¸‰ ì ê²€
        * ì£¼ê°€ê°€ ê¸‰ë“±í–ˆë‹¤ë©´ ê³¼ì—´ ì—¬ë¶€ëŠ” ì—†ëŠ”ì§€, ì•…ì¬(CBë°œí–‰, ëŒ€ì£¼ì£¼ ë§¤ë„ ë“±)ê°€ ìˆ¨ì–´ìˆëŠ”ì§€ ì²´í¬í•˜ì„¸ìš”.
        * í…Œë§ˆ ë‚´ì—ì„œ ì´ ì¢…ëª©ì´ 'ëŒ€ì¥ì£¼'ì¸ì§€ 'í›„ë°œì£¼ì'ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
        
        ### 4. ğŸ’¡ ì‹¤ì „ ë§¤ë§¤ ì „ëµ ë° ì„¸ì¤„ìš”ì•½
        * **í¬ì§€ì…˜:** [ì ê·¹ ë§¤ìˆ˜ / ëˆŒë¦¼ëª© ë§¤ìˆ˜ / ê´€ë§ / ë§¤ë„] ë“± ì „ëµ ì œì‹œ
        * **ì „ëµ:** í˜„ì‹¤ì ì¸ ê°€ì´ë“œë¥¼ ì œì‹œí•˜ì„¸ìš”. (ì˜ˆ: "ì¥ì¤‘ ëŒ€ì‘ í˜ë“œë‹ˆ ì‹œì´ˆê°€ ì´í•˜ ë¶„í•  ë§¤ìˆ˜")
        """
        search_res += f"\n\n[System Instructions]\n{sys_instructions}"
    
    modified_msgs = []
    for i, msg in enumerate(messages):
        content = msg['content']
        if i == len(messages)-1: content += search_res
        modified_msgs.append({"role": "user" if msg['role']=="user" else "model", "parts": [content]})
    
    model = genai.GenerativeModel(f"models/{model_name}")
    try:
        response = model.generate_content(modified_msgs, stream=True)
        for chunk in response: yield chunk.text
    except Exception as e:
        yield f"âš ï¸ API ì˜¤ë¥˜: {str(e)}"

# --- [AI ì‘ë‹µ í•¨ìˆ˜ 2: ì‹œí™© ë¶„ì„ (Macro & Momentum)] ---
# [ë³€ê²½] ì…ë ¥ ë°ì´í„°ì— ìƒìŠ¹ë¥  ìƒìœ„(ëª¨ë©˜í…€) ì¶”ê°€
def analyze_market_macro_v2(df_cap, df_gainers_kospi, df_gainers_kosdaq, news_data, model_name):
    genai.configure(api_key=GOOG_API_KEY)
    model = genai.GenerativeModel(f"models/{model_name}")
    
    # ë°ì´í„° ìš”ì•½ (ìƒìœ„ 50ê°œì”©ë§Œ ì˜ë¼ì„œ ì „ë‹¬ - í† í° ì ˆì•½ ë° í•µì‹¬ íŒŒì•…)
    str_cap = df_cap.head(50).to_string(index=False)
    str_kospi_gain = df_gainers_kospi.head(50).to_string(index=False)
    str_kosdaq_gain = df_gainers_kosdaq.head(50).to_string(index=False)
    
    combined_news = ""
    for item in news_data:
        combined_news += f"[{item['source']}] {item['title']}\n(ìš”ì•½): {item['summary']}\n\n"
    
    prompt = f"""
    ë‹¹ì‹ ì€ ê±°ì‹œê²½ì œì™€ ì‹œì¥ íë¦„ì„ ì½ëŠ” êµ­ë‚´ ìµœê³  'ë§ˆì¼“ìŠ¤íŠ¸ë˜í‹°ì§€ìŠ¤íŠ¸ê²¸ ì• ë„ë¦¬ìŠ¤íŠ¸ ì…ë‹ˆë‹¤

    ê¸´ë§í•˜ì§€ë§ê³  ë°”ë¡œ ë¶„ì„ì— ë“¤ì–´ê°€ ì£¼ì„¸ìš”.
    
    [ì…ë ¥ ë°ì´í„° 3ì¢…]
    1. **Blue Chips (ì‹œì¥ ì§€ìˆ˜ ë°©ì–´):** ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ì‹œê°€ì´ì•¡ ìƒìœ„ 50ìœ„ íë¦„
    {str_cap}
    
    2. **Momentum (ì‹œì¥ ì£¼ë„ í…Œë§ˆ):** ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥ ê¸‰ë“±ì£¼(ìƒìŠ¹ë¥  ìƒìœ„) íë¦„
    [ì½”ìŠ¤í”¼ ê¸‰ë“±]
    {str_kospi_gain}
    [ì½”ìŠ¤ë‹¥ ê¸‰ë“±]
    {str_kosdaq_gain}
    
    3. **News Flow:** ì‹œì¥ ì£¼ìš” ë‰´ìŠ¤ ìš”ì•½ ({len(news_data)}ê±´)
    {combined_news}
    
    [ë¶„ì„ ìš”êµ¬ì‚¬í•­]
    ìœ„ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ 'ëŒ€í˜•ì£¼(ì§€ìˆ˜)'ì™€ 'ê°œë³„ ê¸‰ë“±ì£¼(í…Œë§ˆ)'ì˜ ê´´ë¦¬ë¥¼ ë¶„ì„í•˜ê³ ,
    ì˜¤ëŠ˜ ì‹œì¥ì˜ **'ì§„ì§œ ì£¼ë„ íë¦„'**ì„ ëª…í™•íˆ ì •ì˜í•´ ì£¼ì„¸ìš”.
    
    ### 1. ğŸŒ ì˜¤ëŠ˜ì˜ ì‹œì¥ ì„¸ì¤„ ìš”ì•½ (Market Color)
    * (ì˜ˆ: "ì§€ìˆ˜ëŠ” ë³´í•©ì´ë‚˜ 2ì°¨ì „ì§€ì™€ AI ë¡œë´‡ í…Œë§ˆê°€ í­ë°œí•˜ëŠ” ì¢…ëª© ì¥ì„¸")
    
    ### 2. ğŸ’° ìê¸ˆ íë¦„ ì¶”ì  (Money Flow)
    * **ëŒ€í˜•ì£¼:** ë°˜ë„ì²´, ë°”ì´ì˜¤, ê¸ˆìœµ ë“± ì‹œì´ ìƒìœ„ ì„¹í„°ì˜ ìˆ˜ê¸‰ì€ ì–´ë–»ìŠµë‹ˆê¹Œ?
    * **ê°œë³„ì£¼:** ê¸‰ë“±ì£¼ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë³´ì´ëŠ” **'ì˜¤ëŠ˜ì˜ ê°•ì„¸ í…Œë§ˆ'**ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ? (ì˜ˆ: ì´ˆì „ë„ì²´, í’ˆì ˆì£¼ ë“±)
    
    ### 3. ğŸ“ˆ ì£¼ìš” ê±°ì‹œ ìš”ì¸ ë¶„ì„
    * ë‰´ìŠ¤ì— ì–¸ê¸‰ëœ í™˜ìœ¨, ê¸ˆë¦¬, ë¯¸ ì¦ì‹œ ì˜í–¥, ì •ë¶€ ì •ì±… ë“±ì´ ì˜¤ëŠ˜ ì‹œì¥ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆê¹Œ?
    
    ### 4. ğŸ’¼ íˆ¬ìì ëŒ€ì‘ ê°€ì´ë“œ ë° ì„¸ì¤„ ìš”ì•½
    * ì˜¤ëŠ˜ ê°™ì€ ì¥ì„¸ì—ì„œëŠ” **ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ íˆ¬ì**ê°€ ìœ ë¦¬í•©ë‹ˆê¹Œ? (ëŒíŒŒ ë§¤ë§¤ vs ëˆŒë¦¼ëª© ë§¤ìˆ˜ vs í˜„ê¸ˆ í™•ë³´)
    """
    
    try:
        response = model.generate_content(prompt, stream=True)
        for chunk in response: yield chunk.text
    except Exception as e:
        yield f"âš ï¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ==========================================
# ğŸ–¥ï¸ ë©”ì¸ ì‹¤í–‰
# ==========================================
with st.sidebar:
    st.header("ğŸ” ì„¤ì •")
    if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.rerun()
    
    if GOOG_API_KEY.startswith("AIza"):
        models = get_available_gemini_models(GOOG_API_KEY)
        model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", models, index=0)
        selected_real_name = model_name.split(" ")[1] if " " in model_name else model_name
    else:
        st.error("API í‚¤ í•„ìš”")
        selected_real_name = "gemini-1.5-flash"

# ì´ˆê¸° ë°ì´í„° ë¡œë”©
with st.status("ğŸš€ 3ì¤‘ í•„í„° ë°ì´í„° ë° ì‹œí™© ë°ì´í„° ìˆ˜ì§‘ ì¤‘...", expanded=True) as status:
    # 1. 3ì¤‘ êµì§‘í•©ìš© ë°ì´í„°
    df_themes = get_top_50_themes_stocks() 
    riser_codes = get_risers_codes_only()       
    mf_codes = get_money_flow_codes()
    
    # 2. ì‹œí™© ë¶„ì„ìš© ìƒì„¸ ë°ì´í„° (NEW)
    df_market_cap = get_market_cap_top150() # ì‹œì´ ìƒìœ„ 150
    df_kospi_gainers, df_kosdaq_gainers = get_top_gainers_df(limit=150) # ê¸‰ë“± ìƒìœ„ 150
    
    status.update(label="âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!", state="complete", expanded=False)

tab1, tab2 = st.tabs(["ğŸ¯ 3ì¤‘ êµì§‘í•© ë°œêµ´", "ğŸ“Š ì‹œí™© ë¶„ì„ (Dual-Engine)"])

# --- Tab 1 ---
with tab1:
    st.subheader("1ï¸âƒ£ 3ì¤‘ êµì§‘í•© ë¶„ì„ ê²°ê³¼ (Money Flow Ver.)")
    st.markdown("""
    **í•„í„°ë§ ì¡°ê±´ (AND ì¡°ê±´):**
    1. ğŸ”¥ **í…Œë§ˆ ìƒìœ„ 50ìœ„** ë‚´ ì¢…ëª©
    2. ğŸ“ˆ **ìƒìŠ¹ë¥  ìƒìœ„ 500ìœ„** (ì½”ìŠ¤í”¼+ì½”ìŠ¤ë‹¥)
    3. ğŸ’° **ê±°ë˜ëŒ€ê¸ˆ ìƒìœ„ 500ìœ„** (ì½”ìŠ¤í”¼+ì½”ìŠ¤ë‹¥)
    """)
    
    st.info(f"ğŸ“Š **ë°ì´í„° ìˆ˜ì§‘ í˜„í™©**")
    col1, col2, col3 = st.columns(3)
    col1.metric("ğŸ”¥ í…Œë§ˆ ì¢…ëª©", f"{len(df_themes)}ê°œ")
    col2.metric("ğŸ“ˆ ìƒìŠ¹ ì¢…ëª©", f"{len(riser_codes)}ê°œ")
    col3.metric("ğŸ’° ê±°ë˜ëŒ€ê¸ˆ ì¢…ëª©", f"{len(mf_codes)}ê°œ")
    
    final_candidates = []
    
    if not df_themes.empty:
        for index, row in df_themes.iterrows():
            code = row['code']
            if (code in riser_codes) and (code in mf_codes):
                final_candidates.append(row.to_dict())
                
    if final_candidates:
        df_final = pd.DataFrame(final_candidates)
        df_final = df_final.drop_duplicates(['code'])
        df_final = df_final.sort_values(by="í…Œë§ˆìˆœìœ„")
        
        event = st.dataframe(
            df_final[['í…Œë§ˆìˆœìœ„', 'ì¢…ëª©ëª…', 'í˜„ì¬ê°€(ë“±ë½ë¥ )', 'í…Œë§ˆëª…']], 
            use_container_width=True, 
            hide_index=True, 
            on_select="rerun", 
            selection_mode="single-row"
        )
        
        st.divider()
        
        if len(event.selection.rows) > 0:
            sel_idx = event.selection.rows[0]
            sel_data = df_final.iloc[sel_idx]
            
            s_name = sel_data['ì¢…ëª©ëª…']
            code = sel_data['code']
            s_theme = sel_data['í…Œë§ˆëª…']
            
            if st.session_state.last_code != code:
                st.session_state.messages = []
                st.session_state.last_code = code

            with st.spinner(f"ğŸ” {s_name} ì‹¬ì¸µ ë¶„ì„ ì¤‘ (ë‰´ìŠ¤ 50ê±´)..."):
                fund = get_stock_fundamentals(code)
                news_1 = fetch_google_news_rss(f"{s_name} ì£¼ê°€", limit=25)
                news_2 = fetch_google_news_rss(f"{s_name} í˜¸ì¬ íŠ¹ì§•ì£¼", limit=25)
                
                all_news = news_1 + news_2
                unique_news = {v['link']: v for v in all_news}.values()
                final_news_list = list(unique_news)
                
                market_data_str = f"ì¢…ëª©ëª…: {s_name}\nì½”ë“œ: {code}\ní…Œë§ˆ: {s_theme}\nì‹œê°€ì´ì•¡: {fund['ì‹œê°€ì´ì•¡']}\ní˜„ì¬ê°€(ë“±ë½): {sel_data['í˜„ì¬ê°€(ë“±ë½ë¥ )']}"
            
            st.subheader(f"2ï¸âƒ£ [{s_name}] ìƒì„¸ ë¶„ì„")
            st.info(f"ğŸ’° ì‹œê°€ì´ì•¡: **{fund['ì‹œê°€ì´ì•¡']}** | ğŸ† í…Œë§ˆ: **{s_theme}**")
            
            with st.expander("ğŸ’¬ AI íˆ¬ì ì „ëµê°€ì™€ ëŒ€í™”í•˜ê¸° (Click)", expanded=True):
                if not st.session_state.messages:
                    if st.button(f"âš¡ '{s_name}' ì‹¬ì¸µ ë¶„ì„ ì‹œì‘"):
                        # ì‚¬ìš©ì ìš”ì²­ì—ëŠ” ê°„ë‹¨íˆ
                        st.session_state.messages.append({"role": "user", "content": f"{s_name} ì‹¬ì¸µ ë¶„ì„ ìš”ì²­"})
                        with st.chat_message("assistant"):
                            res_txt = st.write_stream(get_gemini_response_stock_deep(st.session_state.messages, selected_real_name, s_name, s_theme, market_data_str, final_news_list))
                        st.session_state.messages.append({"role": "assistant", "content": res_txt})

                for msg in st.session_state.messages:
                    if msg['role'] == 'user' and "ë‹¹ì‹ ì€" in msg['content']: continue
                    with st.chat_message(msg['role']): st.markdown(msg['content'])

                if prompt := st.chat_input(f"{s_name} ì§ˆë¬¸..."):
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"): st.markdown(prompt)
                    with st.chat_message("assistant"):
                        model = genai.GenerativeModel(f"models/{selected_real_name}")
                        history = [{"role": m["role"], "parts": [m["content"]]} for m in st.session_state.messages]
                        try:
                            res = model.generate_content(history, stream=True)
                            res_txt = st.write_stream(res)
                        except Exception as e:
                            res_txt = f"âš ï¸ ì˜¤ë¥˜: {str(e)}"
                            st.write(res_txt)
                        st.session_state.messages.append({"role": "assistant", "content": res_txt})

            col1, col2 = st.columns([1, 1])
            with col1:
                t1, t2, t3 = st.tabs(["ğŸ“… ì¼ë´‰", "ğŸ“† ì£¼ë´‰", "ğŸ“‹ í…Œë§ˆ ì „ì²´"])
                with t1: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/day/{code}.png", use_container_width=True)
                with t2: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/week/{code}.png", use_container_width=True)
                with t3:
                    cur_theme_list = df_themes[df_themes['í…Œë§ˆëª…']==s_theme]
                    st.dataframe(cur_theme_list[['ì¢…ëª©ëª…','í˜„ì¬ê°€(ë“±ë½ë¥ )']], hide_index=True)
            with col2:
                st.markdown(f"##### ğŸ“° ê´€ë ¨ ë‰´ìŠ¤ (ìƒìœ„ 20ê±´)")
                st.caption(f"â€» ì´ {len(final_news_list)}ê±´ì˜ ë°ì´í„°ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.")
                if final_news_list:
                    for n in final_news_list[:20]: 
                        st.markdown(f"- [{n['title']}]({n['link']})")
                else:
                    st.warning("ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("ì¡°ê±´(í…Œë§ˆ50ìœ„ & ìƒìŠ¹500ìœ„ & ê±°ë˜ëŒ€ê¸ˆ500ìœ„)ì„ ë™ì‹œì— ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ í˜„ì¬ ì—†ìŠµë‹ˆë‹¤.")

# --- Tab 2 ---
with tab2:
    st.header("ğŸ“Š ì‹œì¥ ì…ì²´ ë¶„ì„ (ëŒ€í˜•ì£¼ vs ì£¼ë„ì£¼)")
    
    # [UI ì—…ë°ì´íŠ¸] íƒ­ìœ¼ë¡œ ì‹œì´ìƒìœ„/ê¸‰ë“±ìƒìœ„ êµ¬ë¶„í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
    sub_t1, sub_t2 = st.tabs(["ğŸ¢ ì‹œì´ ìƒìœ„ 150 (ì§€ìˆ˜)", "ğŸš€ ê¸‰ë“± ìƒìœ„ 150 (ëª¨ë©˜í…€)"])
    
    with sub_t1:
        if not df_market_cap.empty:
            st.dataframe(df_market_cap, height=400, use_container_width=True)
    
    with sub_t2:
        c1, c2 = st.columns(2)
        with c1:
            st.markdown("#### ì½”ìŠ¤í”¼ ê¸‰ë“± Top 150")
            if not df_kospi_gainers.empty:
                st.dataframe(df_kospi_gainers, height=400, use_container_width=True)
        with c2:
            st.markdown("#### ì½”ìŠ¤ë‹¥ ê¸‰ë“± Top 150")
            if not df_kosdaq_gainers.empty:
                st.dataframe(df_kosdaq_gainers, height=400, use_container_width=True)
        
    st.divider()
    st.subheader("ğŸ¤– AI ì‹¤ì‹œê°„ ì‹œí™© ë¸Œë¦¬í•‘")
    
    if st.button("ğŸ“¢ ì‹œí™© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì¢…í•© ë¶„ì„ (RSS)"):
        with st.spinner("ì‹œí™© ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
            news_1 = fetch_google_news_rss("í•œêµ­ ì¦ì‹œ ì‹œí™©", limit=20)
            news_2 = fetch_google_news_rss("ì½”ìŠ¤í”¼ ì½”ìŠ¤ë‹¥ íŠ¹ì§•ì£¼", limit=20)
            
            all_market_news = news_1 + news_2
            unique_market_news = {v['link']: v for v in all_market_news}.values()
            final_market_news = list(unique_market_news)
            
        if final_market_news:
            st.success(f"âœ… ë‰´ìŠ¤ {len(final_market_news)}ê±´ í™•ë³´! (ì‹œì´/ê¸‰ë“± ë°ì´í„°ì™€ ê²°í•©í•˜ì—¬ ë¶„ì„ ì‹œì‘)")
            with st.expander("ğŸ” ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° í™•ì¸", expanded=False):
                for n in final_market_news:
                    st.write(f"- {n['title']}: {n['summary']}")
            
            # [AI í˜¸ì¶œ] ì‹œì´ ë°ì´í„° + ê¸‰ë“±ì£¼ ë°ì´í„° + ë‰´ìŠ¤ ë°ì´í„° ëª¨ë‘ ì „ë‹¬
            st.write_stream(analyze_market_macro_v2(df_market_cap, df_kospi_gainers, df_kosdaq_gainers, final_market_news, selected_real_name))
        else:
            st.error("âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨.")
