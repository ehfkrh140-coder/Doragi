import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import google.generativeai as genai
from duckduckgo_search import DDGS
import urllib.parse

# ==========================================
# ğŸ”‘ [í•„ìˆ˜] Gemini API í‚¤ ì„¤ì •
# ==========================================
try:
    GOOG_API_KEY = st.secrets["GOOG_API_KEY"]
except:
    GOOG_API_KEY = "ì—¬ê¸°ì—_í‚¤ë¥¼_ë„£ìœ¼ì„¸ìš”"

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì£¼ì‹ í…Œë§ˆ ë¶„ì„ê¸° (AI Ver.)", layout="wide")
st.title("ğŸ¤– AI ì£¼ì‹ íˆ¬ì ì „ëµê°€ (DuckDuckGo Fix Ver.)")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "last_code" not in st.session_state:
    st.session_state.last_code = None

# --- [ëª¨ë¸ ëª©ë¡] ---
@st.cache_data
def get_available_gemini_models(api_key):
    try:
        genai.configure(api_key=api_key)
        return [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except: return ["gemini-1.5-flash"]

# --- [ë‰´ìŠ¤ ë³¸ë¬¸ ì½ê¸° ê¸°ëŠ¥] ---
def fetch_url_content(url):
    """ë‰´ìŠ¤ ë§í¬ì— ì§ì ‘ ì ‘ì†í•´ì„œ ë³¸ë¬¸ì„ ê¸ì–´ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=3)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        content = " ".join([p.text.strip() for p in paragraphs])
        if len(content) < 50: return None
        return content[:2000] + "..." if len(content) > 2000 else content
    except: return None

# --- [DuckDuckGo ì°¨ë‹¨ ìš°íšŒ ê²€ìƒ‰ í•¨ìˆ˜] ---
def search_news_robust(keyword, limit=15):
    """
    DuckDuckGo ê²€ìƒ‰ (backend='lite' ëª¨ë“œ ì‚¬ìš©í•˜ì—¬ ì°¨ë‹¨ ìš°íšŒ)
    """
    search_context = ""
    results = []
    
    # DDGS ê°ì²´ ìƒì„±
    ddgs = DDGS()
    
    # 1. ë‰´ìŠ¤ íƒ­ ê²€ìƒ‰ ì‹œë„
    try:
        # backend='api'ê°€ ê¸°ë³¸ê°’ì¸ë° ì´ê²Œ ì˜ ë§‰í˜.
        # ì¼ë°˜ text ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹ ìˆœ ì •ë ¬ì„ ì‹œë„í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ì•ˆì •ì ì„
        results = list(ddgs.text(keywords=keyword, region='kr-kr', safesearch='off', backend='lite', max_results=limit))
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
        time.sleep(1)
        try:
             results = list(ddgs.text(keywords=keyword, region='kr-kr', backend='html', max_results=limit))
        except: results = []

    # 2. ê²°ê³¼ ì²˜ë¦¬ (ìƒìœ„ 3ê°œ ë³¸ë¬¸ ì½ê¸°)
    fetched_count = 0
    for i, res in enumerate(results):
        if i >= limit: break
        title = res.get('title', '-')
        link = res.get('href', res.get('url', '')) # backendë§ˆë‹¤ í‚¤ê°’ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
        snippet = res.get('body', res.get('snippet', ''))
        
        full_body = None
        # ìƒìœ„ 3ê°œëŠ” ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‹œë„
        if i < 3 and link:
            full_body = fetch_url_content(link)
        
        if full_body:
            content = f"[ë³¸ë¬¸ë°œì·Œ]: {full_body}"
        else:
            content = f"[ìš”ì•½]: {snippet}"
            
        search_context += f"[DDG-{i+1}] {title}\n{content}\n\n"
        fetched_count += 1
        
    if not search_context: 
        search_context = "DuckDuckGo ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ (ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”)"
        
    return search_context, fetched_count

# --- [ë„¤ì´ë²„ ì‹œí™© ë‰´ìŠ¤ ìˆ˜ì§‘] ---
def get_naver_market_news(limit=15):
    news_context = ""
    try:
        url = "https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://finance.naver.com/'
        }
        res = requests.get(url, headers=headers)
        res.encoding = 'cp949'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        articles = soup.select("dd.articleSubject > a") + soup.select("dt.articleSubject > a")
        summaries = soup.select("dd.articleSummary")
        
        count = 0
        for art, sum_text in zip(articles, summaries):
            if count >= limit: break
            news_context += f"[ë„¤ì´ë²„ì‹œí™©-{count+1}] {art.text.strip()}\n[ë‚´ìš©]: {sum_text.text.strip()}\n\n"
            count += 1
    except Exception as e:
        news_context = f"ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}"
    return news_context

# --- [ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ë“¤] ---
@st.cache_data
def get_naver_themes():
    url = "https://finance.naver.com/sise/theme.naver"
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        data = []
        for row in soup.select("#contentarea_left > table.type_1 > tr"):
            cols = row.select("td")
            if len(cols) >= 4:
                data.append({"í…Œë§ˆëª…": cols[0].text.strip(), "ë§í¬": "https://finance.naver.com" + cols[0].find('a')['href']})
        return pd.DataFrame(data).head(20)
    except: return pd.DataFrame()

def get_theme_details(theme_link):
    headers = {'User-Agent': 'Mozilla/5.0'}
    try:
        res = requests.get(theme_link, headers=headers)
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        stocks = []
        for row in soup.select("table.type_5 > tbody > tr"):
            cols = row.select("td")
            if len(cols) > 4:
                name_tag = cols[0].find('a')
                if not name_tag: continue
                name = name_tag.text.strip()
                link = "https://finance.naver.com" + name_tag['href']
                code_match = re.search(r'code=([0-9]+)', link)
                code = code_match.group(1) if code_match else ""
                price = cols[2].text.strip()
                rate = cols[4].text.strip().replace('\n', '').strip()
                stocks.append({'name': name, 'code': code, 'price_str': f"{price} ({rate})", 'link': theme_link})
        return stocks
    except: return []

@st.cache_data
def get_all_theme_stocks():
    df_themes = get_naver_themes()
    all_stocks = []
    for index, row in df_themes.iterrows():
        stocks_info = get_theme_details(row['ë§í¬'])
        stocks_info.sort(key=lambda x: float(x['price_str'].split('(')[1].replace('%)','').replace('+','').replace('-','-').replace(',','')) if '(' in x['price_str'] else 0, reverse=True)
        for rank, stock in enumerate(stocks_info, 1):
             all_stocks.append({
                 "í…Œë§ˆìˆœìœ„": f"{rank}ìœ„", 
                 "ì¢…ëª©ëª…": stock['name'], 
                 "ì¢…ëª©ì½”ë“œ": stock['code'], 
                 "í…Œë§ˆëª…": row['í…Œë§ˆëª…'], 
                 "í˜„ì¬ê°€(ë“±ë½ë¥ )": stock['price_str']
             })
    return pd.DataFrame(all_stocks)

@st.cache_data
def get_top_risers_info():
    market_map = {}
    headers = {'User-Agent': 'Mozilla/5.0'}
    for s in [0, 1]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_rise.naver?sosok={s}", headers=headers)
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for item in soup.select("table.type_2 tr td a.tltle")[:300]: 
                market_map[item.text.strip()] = "KOSPI" if s==0 else "KOSDAQ"
        except: pass
    return market_map

@st.cache_data
def get_volume_leaders():
    tickers = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    for s in [0, 1]:
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_quant_high.naver?sosok={s}", headers=headers)
            soup = BeautifulSoup(res.text, 'html.parser')
            for item in soup.select("table.type_2 tr td a.tltle")[:200]: 
                tickers.append(item.text.strip())
        except: pass
    return tickers

# [í•µì‹¬ ìˆ˜ì •] ì‹œê°€ì´ì•¡ 'å…†' -> 'ì¡°' í•œê¸€ ì¹˜í™˜
def get_stock_fundamentals(code):
    try:
        url = f"https://finance.naver.com/item/main.naver?code={code}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        cap_elem = soup.select_one("#_market_sum")
        if cap_elem:
            raw_cap = cap_elem.text.strip()
            # å…†(ì¡°) í•œìë¥¼ í•œê¸€ë¡œ ì¹˜í™˜
            raw_cap = raw_cap.replace('å…†', 'ì¡°').replace('ì¡°', 'ì¡° ')
            raw_cap = raw_cap.replace('\t', '').replace('\n', '') + "ì–µ"
            return {"ì‹œê°€ì´ì•¡": raw_cap}
    except: pass
    return {"ì‹œê°€ì´ì•¡": "-"}

def get_latest_news(code):
    try:
        url = f"https://finance.naver.com/item/news_news.naver?code={code}"
        headers = {'User-Agent': 'Mozilla/5.0', 'Referer': f'https://finance.naver.com/item/main.naver?code={code}'}
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
        news_list = []
        
        articles = soup.select(".title > a")
        if not articles: articles = soup.select("a.tit")
        
        for a in articles[:20]:
            title = a.text.strip()
            link = a['href']
            if link.startswith('/'): link = "https://finance.naver.com" + link
            news_list.append({"ì œëª©": title, "ë§í¬": link})
        return news_list
    except: return []

@st.cache_data
def get_market_cap_top150():
    stocks = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    for page in range(1, 4):
        try:
            res = requests.get(f"https://finance.naver.com/sise/sise_market_sum.naver?sosok=0&page={page}", headers=headers)
            soup = BeautifulSoup(res.content.decode('cp949', 'ignore'), 'html.parser')
            for row in soup.select("table.type_2 tbody tr"):
                cols = row.select("td")
                if len(cols) < 10: continue
                stocks.append({
                    "ìˆœìœ„": cols[0].text.strip(), "ì¢…ëª©ëª…": cols[1].text.strip(),
                    "í˜„ì¬ê°€": cols[2].text.strip(), "ë“±ë½ë¥ ": cols[4].text.strip().replace("\n", "").strip(),
                    "ì‹œê°€ì´ì•¡": cols[6].text.strip()
                })
        except: pass
    return pd.DataFrame(stocks)

# --- [AI ì‘ë‹µ í•¨ìˆ˜] ---
def get_gemini_response_robust(messages, model_name, use_search, stock_name, theme):
    genai.configure(api_key=GOOG_API_KEY)
    
    current_query = messages[-1]['content']
    search_res = ""
    
    if use_search and "ë‹¹ì‹ ì€" in current_query:
        with st.spinner(f"ğŸŒ DuckDuckGo ê²€ìƒ‰ ì§„í–‰ ì¤‘... ('{stock_name}')"):
            # ì—¬ê¸°ì„œ DuckDuckGoê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ ë¡œê·¸ë¡œ í™•ì¸ ê°€ëŠ¥
            data, count = search_news_robust(f"{stock_name} {theme} í˜¸ì¬ ì „ë§", limit=5)
            
            if count > 0:
                st.success(f"âœ… DuckDuckGo ê²€ìƒ‰ ì„±ê³µ! ({count}ê±´ ìˆ˜ì§‘)")
                with st.expander("ğŸ” ìˆ˜ì§‘ëœ ë°ì´í„° ì›ë¬¸ ë³´ê¸°"):
                    st.text(data)
            else:
                st.error("âŒ DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨ (ë°ì´í„° 0ê±´). ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                
            search_res = f"\n[DuckDuckGo ê²€ìƒ‰ ë°ì´í„°]:\n{data}\n"
    
    modified_msgs = []
    for i, msg in enumerate(messages):
        content = msg['content']
        if i == len(messages)-1: content += search_res
        modified_msgs.append({"role": "user" if msg['role']=="user" else "model", "parts": [content]})
    
    model = genai.GenerativeModel(f"models/{model_name}")
    response = model.generate_content(modified_msgs, stream=True)
    for chunk in response: yield chunk.text

def analyze_market_trend_ai(df, news_data, model_name):
    genai.configure(api_key=GOOG_API_KEY)
    model = genai.GenerativeModel(f"models/{model_name}")
    top_30 = df.head(30).to_string(index=False)
    
    prompt = f"""
    ë‹¹ì‹ ì€ ìˆ˜ì„ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
    ì œê³µëœ [ì‹œì´ ìƒìœ„ì£¼ ë°ì´í„°]ì™€ [ì‹¤ì‹œê°„ ë‰´ìŠ¤ 30ê±´(ë³¸ë¬¸ í¬í•¨)]ì„ ì² ì €íˆ ë¶„ì„í•˜ì—¬ ì‹œì¥ ìƒí™©ì„ ë¸Œë¦¬í•‘í•˜ì„¸ìš”.

    [ë°ì´í„° ì†ŒìŠ¤ 1: ì½”ìŠ¤í”¼ ì‹œì´ ìƒìœ„ 30ìœ„ íë¦„]
    {top_30}
    
    [ë°ì´í„° ì†ŒìŠ¤ 2: ì‹¤ì‹œê°„ ë‰´ìŠ¤ 30ê±´ (DuckDuckGo + ë„¤ì´ë²„)]
    {news_data}
    
    [ë¶„ì„ ìš”êµ¬ì‚¬í•­]:
    1. ë‰´ìŠ¤ì˜ ë³¸ë¬¸ ë‚´ìš©ê¹Œì§€ ì°¸ê³ í•˜ì—¬ ê¸ˆë¦¬, í™˜ìœ¨, í•´ì™¸ ì¦ì‹œ ë“± ê±°ì‹œì  ìš”ì¸ì„ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
    2. ì‹œì´ ìƒìœ„ì£¼ì˜ ë“±ë½ê³¼ ë‰´ìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ 'ì™œ' ì˜¤ë¥´ê³  ë‚´ë¦¬ëŠ”ì§€ ì¸ê³¼ê´€ê³„ë¥¼ ë°íˆì‹­ì‹œì˜¤.
    3. 34ì„¸ ì§ì¥ì¸ íˆ¬ììë¥¼ ìœ„í•´ êµ¬ì²´ì ì¸ ì„¹í„°ì™€ ëŒ€ì‘ ì „ëµì„ ì œì‹œí•˜ì‹­ì‹œì˜¤.
    """
    response = model.generate_content(prompt, stream=True)
    for chunk in response: yield chunk.text

# ==========================================
# ğŸ–¥ï¸ ë©”ì¸ ì‹¤í–‰
# ==========================================
with st.sidebar:
    st.header("ğŸ” ì„¤ì •")
    if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.rerun()
        
    if GOOG_API_KEY.startswith("AIza"):
        models = get_available_gemini_models(GOOG_API_KEY)
        model_name = st.selectbox("ëª¨ë¸ ì„ íƒ", models, index=0)
        selected_real_name = model_name.split(" ")[1] if " " in model_name else model_name
    else:
        st.error("API í‚¤ í•„ìš”")
        selected_real_name = "gemini-1.5-flash"
    use_grounding = st.checkbox("ğŸŒ ì‹¬ì¸µ ê²€ìƒ‰ ì‚¬ìš©", value=True)

# ì´ˆê¸° ë¡œë”©
with st.status("ğŸš€ ì „ì²´ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘... (êµì§‘í•© + ì‹œì´ + ë‰´ìŠ¤)", expanded=True) as status:
    df_market = get_market_cap_top150()
    market_map = get_top_risers_info()
    vol_leaders = get_volume_leaders()
    df_C = get_all_theme_stocks()
    status.update(label="âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!", state="complete", expanded=False)

tab1, tab2 = st.tabs(["ğŸ¯ ê¸‰ë“±ì£¼ ë°œêµ´", "ğŸ“Š ì‹œí™© ë¶„ì„"])

# --- Tab 1 ---
with tab1:
    st.subheader("1ï¸âƒ£ êµì§‘í•© ë¶„ì„ ê²°ê³¼ (í•µì‹¬ ì£¼ë„ì£¼)")
    list_A = list(market_map.keys())
    list_B = vol_leaders
    final_candidates = []
    
    for index, row in df_C.iterrows():
        stock_name = row['ì¢…ëª©ëª…']
        if (stock_name in list_A) and (stock_name in list_B):
            market_type = market_map.get(stock_name, "Unknown")
            row_data = row.to_dict()
            row_data['ì‹œì¥êµ¬ë¶„'] = market_type
            final_candidates.append(row_data)
            
    if final_candidates:
        df_final = pd.DataFrame(final_candidates)
        df_final = df_final.drop_duplicates(['ì¢…ëª©ëª…'])
        df_final = df_final.sort_values(by="í…Œë§ˆëª…")
        
        event = st.dataframe(
            df_final[['í…Œë§ˆìˆœìœ„', 'ì‹œì¥êµ¬ë¶„', 'ì¢…ëª©ëª…', 'í˜„ì¬ê°€(ë“±ë½ë¥ )', 'í…Œë§ˆëª…']], 
            use_container_width=True, hide_index=True, on_select="rerun", selection_mode="single-row"
        )
        
        st.divider()
        
        if len(event.selection.rows) > 0:
            sel_idx = event.selection.rows[0]
            sel_data = df_final.iloc[sel_idx]
            s_name = sel_data['ì¢…ëª©ëª…']
            code = sel_data['ì¢…ëª©ì½”ë“œ']
            s_theme = sel_data['í…Œë§ˆëª…']
            
            if st.session_state.last_code != code:
                st.session_state.messages = []
                st.session_state.last_code = code

            with st.spinner(f"ğŸ” {s_name} ì •ë³´ ìˆ˜ì§‘ ì¤‘..."):
                fund = get_stock_fundamentals(code)
                news_list = get_latest_news(code)
            
            st.subheader(f"2ï¸âƒ£ [{s_name}] ìƒì„¸ ë¶„ì„")
            # ì‹œê°€ì´ì•¡ í‘œê¸° (í•œê¸€ 'ì¡°' ì ìš©)
            st.info(f"ğŸ’° ì‹œê°€ì´ì•¡: **{fund['ì‹œê°€ì´ì•¡']}** | ğŸ† í…Œë§ˆ: **{s_theme}**")
            
            with st.expander("ğŸ’¬ AI íˆ¬ì ì „ëµê°€ì™€ ëŒ€í™”í•˜ê¸° (Click)", expanded=True):
                if not st.session_state.messages:
                    if st.button(f"âš¡ '{s_name}' ì‹¬ì¸µ ë¶„ì„ ì‹œì‘"):
                        news_ctx = "\n".join([f"- {n['ì œëª©']}" for n in news_list])
                        sys_prompt = f"""
                        ë‹¹ì‹ ì€ ê³µê²©ì ì¸ íˆ¬ì ì „ëµê°€ì…ë‹ˆë‹¤. {s_name}({s_theme})ì„ í˜¸ì¬ ìœ„ì£¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
                        [ë„¤ì´ë²„ ë‰´ìŠ¤ ì œëª©]: {news_ctx}
                        ë°˜ë“œì‹œ 'ğŸš€ í•µì‹¬ í˜¸ì¬ 3ê°€ì§€', 'ğŸ“ˆ í…Œë§ˆ ì „ë§', 'ğŸ’¡ ë§¤ë§¤ ì „ëµ' ìˆœì„œë¡œ ë¸Œë¦¬í•‘í•˜ì„¸ìš”.
                        """
                        st.session_state.messages.append({"role": "user", "content": sys_prompt})
                        with st.chat_message("assistant"):
                            res_txt = st.write_stream(get_gemini_response_robust(st.session_state.messages, selected_real_name, use_grounding, s_name, s_theme))
                        st.session_state.messages.append({"role": "assistant", "content": res_txt})

                for msg in st.session_state.messages:
                    if msg['role'] == 'user' and "ë‹¹ì‹ ì€" in msg['content']: continue
                    with st.chat_message(msg['role']): st.markdown(msg['content'])

                if prompt := st.chat_input(f"{s_name} ì§ˆë¬¸..."):
                    st.session_state.messages.append({"role": "user", "content": prompt})
                    with st.chat_message("user"): st.markdown(prompt)
                    with st.chat_message("assistant"):
                        res_txt = st.write_stream(get_gemini_response_robust(st.session_state.messages, selected_real_name, use_grounding, s_name, s_theme))
                    st.session_state.messages.append({"role": "assistant", "content": res_txt})

            col1, col2 = st.columns([1, 1])
            with col1:
                t1, t2, t3 = st.tabs(["ğŸ“… ì¼ë´‰", "ğŸ“† ì£¼ë´‰", "ğŸ“‹ í…Œë§ˆ ì „ì²´"])
                with t1: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/day/{code}.png", use_container_width=True)
                with t2: st.image(f"https://ssl.pstatic.net/imgfinance/chart/item/candle/week/{code}.png", use_container_width=True)
                with t3:
                    cur_theme_list = df_C[df_C['í…Œë§ˆëª…']==s_theme]
                    st.dataframe(cur_theme_list[['í…Œë§ˆìˆœìœ„','ì¢…ëª©ëª…','í˜„ì¬ê°€(ë“±ë½ë¥ )']], hide_index=True)
            with col2:
                st.markdown(f"##### ğŸ“° ìµœì‹  ë‰´ìŠ¤ ({len(news_list)}ê±´)")
                for i, n in enumerate(news_list):
                    st.markdown(f"{i+1}. [{n['ì œëª©']}]({n['ë§í¬']})")
    else:
        st.warning("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

# --- Tab 2 ---
with tab2:
    st.header("ğŸ“Š ì‹œì¥ ì „ì²´ íë¦„ (ì‹œì´ Top 150)")
    if df_market is not None:
        st.dataframe(df_market, height=400)
        
        st.subheader("ğŸ¤– AI ì‹¤ì‹œê°„ ì‹œí™© ë¸Œë¦¬í•‘")
        if st.button("ğŸ“¢ ë‰´ìŠ¤ 30ê°œ(DDG+Naver) ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘"):
            # 1. DuckDuckGo 15ê°œ
            with st.spinner("1. DuckDuckGo ê²€ìƒ‰ ì¤‘ (lite ëª¨ë“œ)..."):
                ddg_data, ddg_cnt = search_news_robust("ê¸ˆì¼ ì½”ìŠ¤í”¼ ì½”ìŠ¤ë‹¥ ì‹œí™© íŠ¹ì§•ì£¼", limit=15)
            
            # 2. ë„¤ì´ë²„ ì‹œí™© 15ê°œ
            with st.spinner("2. ë„¤ì´ë²„ ì‹œí™© ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                naver_data = get_naver_market_news(limit=15)
            
            combined_news = f"--- [DuckDuckGo] ---\n{ddg_data}\n\n--- [ë„¤ì´ë²„ ì‹œí™©] ---\n{naver_data}"
            
            # ë¡œê·¸ ì¶œë ¥
            if ddg_cnt > 0: st.success(f"âœ… DuckDuckGo {ddg_cnt}ê±´ + ë„¤ì´ë²„ 15ê±´ ìˆ˜ì§‘ ì™„ë£Œ!")
            else: st.warning("âš ï¸ DuckDuckGo ìˆ˜ì§‘ ì‹¤íŒ¨ (ë„¤ì´ë²„ ë°ì´í„°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤)")

            with st.expander(f"ğŸ” AIê°€ ì°¸ê³ í•œ ë‰´ìŠ¤ ì›ë¬¸ ë³´ê¸°", expanded=True):
                st.text(combined_news)
                
            with st.spinner("3. AI ë¶„ì„ ì¤‘..."):
                st.write_stream(analyze_market_trend_ai(df_market, combined_news, selected_real_name))
